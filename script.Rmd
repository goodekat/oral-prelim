---
title: "Script for Oral Prelim"
author: "Katherine Goode"
date: "May 6, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Slide 1: Opening Statement

- Thank for being on committee and here
- Welcome to ask questions at any time
- I'm going to get started unless there is anything else

# Slide 2: Personal Background

- Brief overview of background 
- Go through education
- Go through assisstantships

# Slide 3: Overview of Talk

- Go over plan
- Bring up discussion topic of publication of my first chapter

# Slide 5: Explainable Machine Learning

- All chapters focused on visualziations for explainable machine learning
- Provide some background
- Read the quote and explain its importance

# Slide 6: Explainability versus Interpretability

- Before getting into a discuss of approached taken for explainable machine learning, I want to discuss an important issue in explainable machine leraning, and that is the definitions of explanability and interpretability
- There have been many papers that define these terms, but there are no currently accepted defintions
- Some people use the terms interchangeable, and other like to make a distinction.
- I am in the distinction boat, and here are the definitions that I like to use.
- I haven't found a paper that explicity defines these terms, but the papers I have listed here by Rudin and Ribeiro, Singh, and Guestri implicity use these definitions.
- I'll go ahead and read the my definitions:
- *Read interpretability*
- My example here is a linear regresion model since we can use the coefficients to say exactly how the resopnse variable is affected by a change in a predictor variable (for all other predictors held constant)
- In contrast, *read explainability*
- My example here is the explainabile machine leraning method of LIME, which uses an interpretable model to try to explain a black-box model by focusing on a specific region in the feature space of the black-box model
- This image is from the original paper, and you can see how a linear model has been used to identify the complex model relationship in a certain region even though it doesn't explain the who complex model classification boundary

# Slide 7: Model Agnostic Methods

- Now I wanted to get into an overview of methods used for explainable machine learning (I try to focus on visualization methods)
- It's possible to divide the methods into two main categories, which are model agnostic methods (those that work with any model) and model specific methods (those designed to work with a specific type of model)
- I'll start with model agnostic methods
- On this slide and the next, I've listed some of these methods. This is definitiely not an exhaustive list, but it gives an idea of the methods out there. - I tried to divide these into subcategories to try to provide an overview. Even though - some could probably fit under several of these categories.
- First, I have a category for general model visualzitions. These are strategies for understanding any model (black-box or not), and I just have one paper included here, which is the Removing the Blindfold by Hadely, Heike, and Di Cook. The top figure is from this paper and is an example of plotting the model in the data-space where the data has been plotted with the model classification boundaries to see how these relate 
- Next, I have a subcategory for global methods. These are methods that intend to provide an explanation for a model as a whole. These include methods such as partial dependence plots, which I'm guessing you are all familiar with, and global surrogate models, which are methods that use an interpretable model to understand the relationshp between a black-box model's predictions and the features. The bottom figure is an example of a partial dependence plot that includes two features. 

# Slide 8: Model Agnostic Methods

- Then there are local methods, which are methods that focus on explaining one prediction at a time. These include LIME and Shapley values. The top figrue on this slide is an example of shapely values computed for the features from a model for one prediction of interest. I don't want to get into the specifics, but the bigger the bar, the more important the feature is in that specific prediction. if you made this plot for a different prediction, you may find that different variables are important.
- Lastely, I have exmpale based methods, which try to explain the model by using examples from teh data. The bottom fiegure is an example of an adversairal example where the authors changed one pixel in these images, and the model changed to incorrect classifications.

# Slide 9: Model Specific Methods

- Here are some example of model specific methods
- I am just focusing on random forests and neural networks, becaue those are the models I am interested in for my second and third chapters
- I have included an example visualization from Urbanek 2008, which is a sectioned scatterplot. It plots all of the tree splits from the random forest that related to those two variables. 
- On the bottom is a method specific to neural networks called saliency maps, which uses the gradients to identify key pixels (in terms of images) or features (in general) that are important in the classification.

# Slide 10: Assessments of Explainable Machine Learning

- In addition to all of the research that proposes methods for explanations, some research is focused on assessing these methods
- First, I need to mention the paper by Rudin from 2018, which is titled "Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead"
- The title pretty much says it all
- Rudin is of the opinion that there are enough issues with explanation methods such as explanations not being faithfulto the original model or detailed enough to explain the complex model, that we should give up on trying to explain these models and use interpretable models instead
- She also focuses on dubunking the myth that an increase in interpretability decreases predictive ability
- In addition to this general assessment, there are some specific ones
- Here I've listed three that I've come across that assess LIME, counterfactual examples, and saliency maps

# Slide 11: Overview of Dissertation Chapters

- Here is a list of my chapters
- The first one focuses on providing visual diagnostics to assess the mehtod of LIME. When trying to use LIME to provide explnations for a random forest model, we ran into many of the issues with explainers that Rundin brings up. This led us to suggest the use of visualizations to assess LIME explanations.
- For the second chapter, we would like to focus on creating some visualization methods for explainainb random forests that focus on identifying key trees in the model. I'll say more about this later in the talk.
- The content of the third chapter is motivated by an application from Sandia National Labs and the desire to both undersatnd and explain neural networks

- Berfore I get into the content of my first chapter, are there any quetions right now?

# Slide 13: Motivation

- As I briefly mentioned, this project was motivated by the desire to explain a random forest model. 
- In particular, this is the random forest model from CSAFE bullet matching reserach by Eric Hare, Heike, and Alicia.
- I'm guessing that many of you are familair with this, but I'll go through the application quickly. 
- The original goal was to provide a quantitative measure for determining whether two bullets were fired from the same gun or not. When a bullet basses through a gun, small indentations are left on the bullet. You can see the markings on the inside of the gun in this image, and this image shows the markings on the bullet. - THe bottom right image shows two signatures taht Hare, Hofmann, and Carriquiry extracted from two bullets. They use these to compare two bullets. 

# Slide 14: Motivation

- They developed nine features to compare the similarity between the signatures, and fit a random forest model. 
- I created a parallel coordinate plot of the random forest model that I have included here. 
- The top row includes the training data, and the bottom row includes the testing set. The features are on the x-axis ordered by feature importance from the random forest model, and the standardized feature values are shown in the y-axis. 
- The lines represent on observation in the dataset, and the color represents the random forest probability.
- You can see the division in random forest predictions based on feature value

# Slide 15: Motivation

- My role was originally to apply the method of LIME to this random forest model 
- We were hoping for a cute and short paper, but it didn't turn out so well
- We ended up getting some unusual results
- Here is an image of a LIME explanation (and I don't want to get into the detail yet - we'll get there in a little bit)
- What I want you to understand for right now is that this is a LIME explanation for one prediction
- The bars indicate which features are important in the predciotn and whether they support a classification of a match or not - we wee that the first variable of rough correlation should support there being a match - same with the third variable of sum peaks and the second variable contradicts a match
- Now, this case, we know to be a non-match, and the probabiliyt listed ehre (0.05) is the random forest probability
- The random forest got it right!
- however, LIME is telling us that this two of the features that were important in the random forest prediction support a classification of a match, but the random forest doesnt think it is a match
- What??!!!!
- This led to the development of a handful of visual diagnostics to try to understand what is going on with LIME

# Slide 16: Conceptual Depiction of LIME

- Now let's finally get into more details about LIME
- LIME was introduced by Ribeiro, Singh, and Guestrin in 2016
- It stands for Local Interpretable Model-agnostic explanations
- LIME is meant to provide an explanation for one prediction of interest by focusing on a local neigbhorhood around the preidciton of interst and using an interpretable model to capture the relationship between the black-box preditions and the features 
- The figure is an example of how LIME works
- Black-box predictions plotted on the y-axis and two hypothetical features used to fit the model
- The diamond shaped point is the prediction of interest to explain and the black lines represent the 
