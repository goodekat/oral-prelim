@article{review19,
author = {VanderPlas, Susan and Cook, Dianne and Hofmann, Heike},
title = {{Testing Statistical Charts: What Makes a Good Graph?}},
journal = {Annual Review of Statistics and Its Application},
year = {2020},
volume = {7},
number = {1},
pages = {61--88},
month = mar,
doi = {10.1146/annurev-statistics-031219-041252},
language = {English},
rating = {0},
date-added = {2020-04-22T13:55:56GMT},
date-modified = {2020-04-22T13:56:15GMT},
url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-031219-041252},
uri = {\url{papers3://publication/doi/10.1146/annurev-statistics-031219-041252}}
}


@article{simonsChangeBlindness1997,
	title = {Change blindness},
	volume = {1},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661397010802},
	doi = {10.1016/S1364-6613(97)01080-2},
	abstract = {Although at any instant we experience a rich, detailed visual world, we do not use such visual details to form a stable representation across views. Over the past five years, researchers have focused increasingly on ‘change blindness’ (the inability to detect changes to an object or scene) as a means to examine the nature of our representations. Experiments using a diverse range of methods and displays have produced strikingly similar results: unless a change to a visual scene produces a localizable change or transient at a specific position on the retina, generally, people will not detect it. We review theory and research motivating work on change blindness and discuss recent evidence that people are blind to changes occurring in photographs, in motion pictures and even in real-world interactions. These findings suggest that relatively little visual information is preserved from one view to the next, and question a fundamental assumption that has underlain perception research for centuries: namely, that we need to store a detailed visual representation in the mind/brain from one view to the next.},
	number = {7},
	urldate = {2019-06-28},
	journal = {Trends in Cognitive Sciences},
	author = {Simons, Daniel J. and Levin, Daniel T.},
	month = oct,
	year = {1997},
	pages = {261--267},
}
@article{alvarezmelis:2018, 
  author = {Alvarez-Melis, David and Jaakkola, Tommi S}, 
  title = {{On the Robustness of Interpretability Methods}}, 
  eprint = {1806.08049}, 
  abstract = {{We argue that robustness of explanations---i.e., that 
  similar inputs should give rise to similar explanations---is a key 
  desideratum for interpretability. We introduce metrics to quantify 
  robustness and demonstrate that current methods do not perform well
  according to these metrics. Finally, we propose ways that robustness
  can be enforced on existing interpretability approaches.}}, 
  keywords = {LIME diagnostics paper,phd}, 
  year = {2018}
}


@article{bau:2017, 
  author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, 
  Aude and Torralba, Antonio}, 
  title = {{Network Dissection: Quantifying Interpretability of Deep
  Visual Representations}}, 
  eprint = {1704.05796}, 
  abstract = {{We propose a general framework called Network Dissection 
  for quantifying the interpretability of latent representations of CNNs
  by evaluating the alignment between individual hidden units and a set 
  of semantic concepts. Given any CNN model, the proposed method draws on
  a broad data set of visual concepts to score the semantics of hidden 
  units at each intermediate convolutional layer. The units with semantics
  are given labels across a range of objects, parts, scenes, textures, 
  materials, and colors. We use the proposed method to test the hypothesis 
  that interpretability of units is equivalent to random linear combinations 
  of units, then we apply our method to compare the latent representations
  of various networks when trained to solve different supervised and 
  self-supervised training tasks. We further analyze the effect of training
  iterations, compare networks trained with different initializations, 
  examine the impact of network depth and width, and measure the effect 
  of dropout and batch normalization on the interpretability of deep
  visual representations. We demonstrate that the proposed method can
  shed light on characteristics of CNN models and training methods that go 
  beyond measurements of their discriminative power.}}, 
  year = {2017}
}

@article{friedman:2001, 
  author = {Friedman, Jerome H.}, 
  title = {{Greedy function approximation: A gradient boosting machine.}}, 
  issn = {0090-5364}, 
  doi = {10.1214/aos/1013203451}, 
  abstract = {{Function estimation/approximation is viewed from the 
  perspective ofnumerical optimization in function space, rather than 
  parameter space. A connection is made between stagewise additive 
  expansions and steepest-descent minimization. A general gradient 
  descent “boosting” paradigm is developed for additive expansions
  based on any fitting criterion.Specific algorithms are presented 
  for least-squares, least absolute deviation, and Huber-M loss 
  functions for regression, and multiclass logistic likelihood for 
  classification. Special enhancements are derived for the particular 
  case where the individual additive components are regression trees, 
  and tools for interpreting such “TreeBoost” models are presented.
  Gradient boosting of regression trees produces competitive, highly 
  robust, interpretable procedures for both regression and classification, 
  especially appropriate for mining less than clean data. Connections
  between this approach and the boosting methods of Freund and Shapire 
  and Friedman, Hastie and Tibshirani are discussed.}}, 
  number = {5}, 
  volume = {29}, 
  journal = {The Annals of Statistics}, 
  keywords = {LIME diagnostics paper}, 
  year = {2001}
}

@article{goodman:2016, 
  title = {{European Union regulations on algorithmic decision-making
  and a "right to explanation"}}, 
  author = {Goodman, Bryce and Flaxman, Seth}, 
  abstract = {We summarize the potential impact that the European
  Union's new General Data Protection Regulation will have on the
  routine use of machine learning algorithms. Slated to take effect
  as law across the EU in 2018, it will restrict automated individual
  decision-making (that is, algorithms that make decisions based on
  user-level predictors) which "significantly affect" users. The law
  will also effectively create a "right to explanation," whereby a user
  can ask for an explanation of an algorithmic decision that was made
  about them. We argue that while this law will pose large challenges
  for industry, it highlights opportunities for computer scientists to
  take the lead in designing algorithms and evaluation frameworks which
  avoid discrimination and enable explanation.}, 
  doi = {10.1609/aimag.v38i3.2741}, 
  arxiv = {1606.08813}, 
  year = {2016}, 
  keywords = {\#machine learning explanations}
}

@article{gower:1971, 
  title = {{A General Coefficient of Similarity and Some of Its Properties}}, 
  author = {Gower, J. C.}, 
  journal = {Biometrics}, 
  volume = {27}, 
  pages = {857--871}, 
  url = { https://www.jstor.org/stable/2528823}, 
  doi = {10.2307/2528823}, 
  read = {false}, 
  year = {1971}, 
  keywords = {\#LIME}
}

@article{greenwell-2018, 
  author = {Greenwell, Brandon M and Boehmke, Bradley C and McCarthy, Andrew J}, 
  title = {{A Simple and Effective Model-Based Variable Importance Measure}}, 
  eprint = {1805.04755}, 
  abstract = {{In the era of "big data", it is becoming more of a challenge 
  to not only build state-of-the-art predictive models, but also gain an
  understanding of what's really going on in the data. For example, it is
  often of interest to know which, if any, of the predictors in a fitted
  model are relatively influential on the predicted outcome. Some modern 
  algorithms---like random forests and gradient boosted decision 
  trees---have a natural way of quantifying the importance or relative 
  influence of each feature. Other algorithms---like naive Bayes classifiers 
  and support vector machines---are not capable of doing so and model-free 
  approaches are generally used to measure each predictor's importance. In
  this paper, we propose a standardized, model-based approach to measuring
  predictor importance across the growing spectrum of supervised learning 
  algorithms. Our proposed method is illustrated through both simulated and
  real data examples. The R code to reproduce all of the figures in this 
  paper is available in the supplementary materials.}}, 
  year = {2018}
}

@article{guidotti:2018, 
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore 
  and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca}, 
  title = {{A Survey Of Methods For Explaining Black Box Models}}, 
  eprint = {1802.01933}, 
  abstract = {In the last years many accurate decision support systems
  have been constructed as black boxes, that is as systems that hide
  their internal logic to the user. This lack of explanation constitutes
  both a practical and an ethical issue. The literature reports many
  approaches aimed at overcoming this crucial weakness sometimes at the
  cost of scarifying accuracy for interpretability. The applications in
  which black box decision systems can be used are various, and each
  approach is typically developed to provide a solution for a specific
  problem and, as a consequence, delineating explicitly or implicitly its
  own definition of interpretability and explanation. The aim of this
  paper is to provide a classification of the main problems addressed in
  the literature with respect to the notion of explanation and the type
  of black box system. Given a problem definition, a black box type, and
  a desired explanation this survey should help the researcher to find
  the proposals more useful for his own work. The proposed classification
  of approaches to open black box models should also be useful for
  putting the many research open questions in perspective.}, 
  keywords = {\#machine learning explanations}, 
  year = {2018}
}

@article{hamby:2009, 
  author = {Hamby, James E. and Brundage, David J. and Thorpe, James W.},
  title = {{The Identification of Bullets Fired from 10 Consecutively
  Rifled 9mm Ruger Pistol Barrels: A Research Project Involving 507
  Participants from 20 Countries}}, 
  abstract = {{Ten consecutively rifled RUGER P-85 pistol barrels 
  were obtained from the manufacturer and then test fired to produce
  known test bullets and ‘unknown’ bullets for comparison by firearms
  examiners from around the world. This study is a continuation of one
  originally designed and reported on by David Brundage [1]. The original
  study was primarily limited to examiners from nationally accredited
  laboratories in the United States.  For this study, the sets were
  provided to  fire-arms examiners around the world. The RUGER P-85
  pistol and the 10 consecutively rifled barrels used for the original
  study were borrowed from the Illinois State Police. Ammunition was
  obtained from the Winchester Ammunition Company (A Division of Olin)
  and 240 tests sets were produced and distributed to forensic scientists
  and researchers worldwide. A thesis which involved a total of 201
  participants – including the original 67 reported on by Brundage – was
  published by Hamby and Thorpe in 2001 [2]. This paper reports the final
  conclusions of the research conducted by Brundage, Hamby and Thorpe
  over a 10 year period [3, 4]. }}, 
  pages = {99--110}, 
  number = {2}, 
  volume = {41}, 
  journal = {AFTE Journal}, 
  keywords = {bullets}, 
  local-url = {file://localhost/Users/katherinegoode/Documents/Papers%20Library/Hamby-2009-AFTE%20Journal.pdf}, 
  year = {2009}
}

@article{hara:2016, 
  author = {Hara, Satoshi and Hayashi, Kohei}, 
  title = {{Making Tree Ensembles Interpretable}}, 
  eprint = {1606.05390}, 
  abstract = {{Tree ensembles, such as random forest and boosted trees,
  are renowned for their high prediction performance, whereas their 
  interpretability is critically limited. In this paper, we propose a 
  post processing method that improves the model interpretability of 
  tree ensembles. After learning a complex tree ensembles in a standard 
  way, we approximate it by a simpler model that is interpretable for 
  human. To obtain the simpler model, we derive the EM algorithm
  minimizing the KL divergence from the complex ensemble. A synthetic
  experiment showed that a complicated tree ensemble was approximated 
  reasonably as interpretable.}}, 
  year = {2016}
}

@article{hare:2016,
	Archiveprefix = {arXiv},
	Author = {{Hare}, Eric and {Hofmann}, Heike and {Carriquiry}, Alicia},
	Doi = {http://adsabs.harvard.edu/abs/2016arXiv160105788H},
	Eprint = {1601.05788},
	Journal = {Annals of Applied Statistics},
	Keywords = {Statistics - Applications},
	Month = jan,
	Primaryclass = {stat.AP},
	Title = {{Automatic Matching of Bullet Lands}},
	Year = 2016,
	Bdsk-Url-1 = {http://adsabs.harvard.edu/abs/2016arXiv160105788H}}

@article{hummel:1996,
   author =  {J\"urgen Hummel},
   title =   {Linked Bar Charts: Analysing Categorical Data Graphically.},
   year =    1996,
   pages =   {23-33},
   journal = {Journal of Computational Statistics},
   volume =  11,
}

@article{kuhn:2008,
   author = {Max Kuhn},
   title = {Building Predictive Models in R Using the caret Package},
   journal = {Journal of Statistical Software, Articles},
   volume = {28},
   number = {5},
   year = {2008},
   keywords = {},
   abstract = {The caret package, short for classification and
   regression training, contains numerous tools for developing
   predictive models using the rich set of models available in R. The
   package focuses on simplifying model training and tuning across a
   wide variety of modeling techniques. It also includes methods for
   pre-processing training data, calculating variable importance, and
   model visualizations. An example from computational chemistry is
   used to illustrate the functionality on a real data set and to
   benchmark the benefits of parallel processing with several types of
   models.},
   issn = {1548-7660},
   pages = {1--26},
   doi = {10.18637/jss.v028.i05},
   url = {https://www.jstatsoft.org/v028/i05}
}

@article{laugel:2018,
  author = {Thibault Laugel and
            Xavier Renard and
            Marie{-}Jeanne Lesot and
            Christophe Marsala and
            Marcin Detyniecki},
  title = {Defining Locality for Surrogates in Post-hoc Interpretablity},
  journal = {CoRR},
  volume = {abs/1806.07498},
  year = {2018},
  url = {http://arxiv.org/abs/1806.07498},
  archivePrefix = {arXiv},
  eprint = {1806.07498},
  timestamp = {Mon, 13 Aug 2018 16:49:13 +0200},
  biburl = {https://dblp.org/rec/bib/journals/corr/abs-1806-07498},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Manual{pedersen:2020,
    title = {lime: Local Interpretable Model-Agnostic Explanations},
    author = {Thomas Lin Pedersen and Michaël Benesty},
    year = {2020},
    note = {https://lime.data-imaginist.com, https://github.com/thomasp85/lime},
  }

@article{mohseni:2018, 
  author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D}, 
  title = {{A Survey of Evaluation Methods and Measures for Interpretable 
  Machine Learning}}, 
  eprint = {1811.11839}, 
  abstract = {The need for interpretable and accountable intelligent 
  system gets sensible as artificial intelligence plays more role in human
  life. Explainable artificial intelligence systems can be a solution by
  self-explaining the reasoning behind the decisions and predictions of the
  intelligent system. Researchers from different disciplines work together
  to define, design and evaluate interpretable intelligent systems for the
  user. Our work supports the different evaluation goals in interpretable
  machine learning research by a thorough review of evaluation 
  methodologies used in machine-explanation research
  across the fields of human-computer interaction, visual analytics, and
  machine learning. We present a 2D categorization of interpretable 
  machine learning evaluation methods and show a mapping between user
  groups and evaluation measures. Further, we address the essential
  factors and steps for a right evaluation plan by proposing a nested 
  model for design and evaluation of explainable artificial intelligence
  systems.}, 
  keywords = {\#machine learning explanations}, 
  year = {2018}
}

@book{molnar:2019, 
  author = {Christopher Molnar}, 
  title = {{Interpretable Machine Learning}}, 
  keywords = {\#machine learning explanations}, 
  year = {2019}
}

@article{nrc:2009, 
  author = {Community, Committee on Identifying the Needs of the 
  Forensic Sciences and Council, National Research}, 
  title = {{Strengthening Forensic Science in the United States: 
  A Path Forward}}, 
  journal = {National Academies Press}, 
  keywords = {bullets}, 
  year = {2009}
}

@online{pcast:2016,
  title = {Report on Forensic Science in Criminal Courts: Ensuring
  Scientific Validity of Feature-Comparison Methods},
  author = {{President's Council of Advisors on Science and Technology}},
  year = {2016},
  url = {https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf},
  urldate = {2016-09-24}
}

@inproceedings{ribeiro:2016,
  author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  title = {"Why Should {I} Trust You?": Explaining the Predictions of
  Any Classifier},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International
  Conference on Knowledge Discovery and Data Mining, San Francisco, CA,
  USA, August 13-17, 2016},
  pages = {1135--1144},
  year = {2016},
}

@article{sapra:2014, 
  author = {Sapra, R.L.}, 
  title = {{Using R2 with caution}}, 
  issn = {2352-0817}, 
  doi = {10.1016/j.cmrp.2014.06.002}, 
  abstract = {R2 has been in use over the several decades to test the
  goodness of fit of regression models. However, it has been under
  criticism over the last three to four decades owing to its diverse
  limitations as well as its appropriateness for its applicability to
  nonlinear models. The paper reviews some of the significant work done,
  especially with respect to its strength and weaknesses, and cautions 
  the users of this widely used statistic in the predictive methodology.
  The paper also makes a brief mention of the alternative statistics.}, 
  pages = {130--134}, 
  number = {3}, 
  volume = {4}, 
  journal = {Current Medicine Research and Practice}, 
  year = {2014}
}

@Article{simon:2011,
  title = {Regularization Paths for Cox's Proportional Hazards Model via
  Coordinate Descent},
  author = {Noah Simon and Jerome Friedman and Trevor Hastie and
  Rob Tibshirani},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {39},
  number = {5},
  pages = {1--13},
  url = {http://www.jstatsoft.org/v39/i05/},
}

@article{strumbelj:2014, 
  author = {Štrumbelj, Erik and Kononenko, Igor}, 
  title = {{Explaining prediction models and individual predictions 
  with feature contributions}}, 
  issn = {0219-1377}, 
  doi = {10.1007/s10115-013-0679-x}, 
  abstract = {{We present a sensitivity analysis-based method for 
  explaining prediction models that can be applied to any type of
  classification or regression model. Its advantage over existing 
  general methods is that all subsets of input features are perturbed,
  so interactions and redundancies between features are taken into 
  account. Furthermore, when explaining an additive model, the method
  is equivalent to commonly used additive model-specific methods.
  We illustrate the method’s usefulness with examples from artificial 
  and real-world data sets and an empirical analysis of running times.
  Results from a controlled experiment with 122 participants suggest 
  that the method’s explanations improved the participants’ 
  understanding of the model.}}, 
  pages = {647--665}, 
  number = {3}, 
  volume = {41}, 
  journal = {Knowledge and Information Systems}, 
  keywords = {LIME diagnostics paper}, 
  year = {2014}
}

@article{wickham:2015, 
  author = {Wickham, Hadley and Cook, Dianne and Hofmann, Heike}, 
  title = {{Visualizing statistical models: Removing the blindfold}}, 
  issn = {1932-1872}, 
  doi = {10.1002/sam.11271}, 
  abstract = {{Visualization can help in model building, diagnosis, 
  and in developing an understanding about how a model summarizes data.
  This paper proposes three strategies for visualizing statistical 
  models: (i) display the model in the data space, (ii) look at all 
  members of a collection, and (iii) explore the process of model 
  fitting, not just the end result. Each strategy is accompanied by
  examples, including manova, classification algorithms, hierarchical
  clustering, ensembles of linear models, projection pursuit, 
  self-organizing maps, and neural networks.}}, 
  pages = {203--225}, 
  number = {4}, 
  volume = {8}, 
  journal = {Statistical Analysis and Data Mining: The ASA Data 
  Science Journal}, 
  keywords = {LIME diagnostics paper}, 
  year = {2015}
}

@article{yu:2018, 
  title = {{Artificial intelligence in healthcare}}, 
  author = {Yu, Kun-Hsing and Beam, Andrew L. and Kohane, Isaac S.}, 
  journal = {Nature Biomedical Engineering}, 
  abstract = {This Review summarizes the medical applications of 
  artificial intelligence, and its economic, legal and social 
  implications for healthcare.},
  volume = {2}, 
  pages = {719}, 
  issn = {2157-846X}, 
  eissn = {2157-846X}, 
  doi = {10.1038/s41551-018-0305-z}, 
  year = {2018}
}
