<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Visualization Methods for Explainable Machine Learning   Statistics PhD Oral Prelim</title>
    <meta charset="utf-8" />
    <meta name="author" content="Katherine Goode" />
    <meta name="date" content="2020-05-06" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <strong>Visualization Methods for Explainable Machine Learning</strong> <br> Statistics PhD Oral Prelim
### Katherine Goode
### Iowa State University
### May 6, 2020

---


&lt;style&gt;

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #7C2529;
  font-size: 20px;
  font-weight: 300;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.inverse {
  background-color: #6E6259;
  border-top: 80px solid #6E6259;
  text-shadow: none;
}

.inverse-sub {
  background-color: #707372;
  border-top: 80px solid #707372;
  text-shadow: none;
}
.inverse-sub h1, .inverse-sub h2, .inverse-sub h3 {
  color: #f3f3f3;
}

.remark-slide-content &gt; h1 {
  font-family: 'Songti SC';
  font-weight: normal;
  font-size: 45px;
  margin-top: -95px;
  margin-left: -00px;
  color: #FFFFFF;
}

.title-slide {
  background-color: #FFFFFF;
  border-top: 80px solid #7C2529;
  border-bottom: 20px solid #6E6259;
}

.title-slide &gt; h1  {
  color: #1A292C;
  font-size: 40px;
  text-shadow: none;
  font-weight: 400;
  text-align: left;
  margin-left: 15px;
  padding-top: 80px;
}

body {
  font-family: 'Songti SC';
}

.remark-slide-number {
  font-size: 13pt;
  font-family: 'Songti SC';
  color: #272822;
  opacity: 1;
}

.inverse .remark-slide-number {
  font-size: 13pt;
  font-family: 'Songti SC';
  color: #FAFAFA;
  opacity: 1;
}

.inverse-sub .remark-slide-number {
  font-size: 13pt;
  font-family: 'Songti SC';
  color: #FAFAFA;
  opacity: 1;
}


a, a &gt; code {
  color: #000000; /*default: rgb(249, 38, 114); || sets color of links */
  text-decoration: none; /* turns off background coloring of links */
}

.left-desc {
  width: 40%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 59%;
  float: right;
  padding-left: 1%;
}

.tiny{font-size: 30%}
.small{font-size: 50%}
.medium{font-size: 75%}
.large{font-size: 85%}
.red{color: #9A3324}
.blue{color: #006BA6}
.tan{color: #9B945F}
.grey{color: #707372}
.orange{color: #FAAA72}
.darkgrey{color: }
&lt;/style&gt;



# Personal Background

.pull-left[
**Education**

B.A. in Mathematics
  - Lawrence University (Appleton, WI)
  - Graduated in June 2013
  
M.S. in Statistics 
  - University of Wisconsin, Madison
  - Graduated in May 2015
  
Ph.D. in Statistics (in progress)
  - Iowa State University
  - Started in January 2016
]

.pull-right[

**Teaching**

- Teaching assistant at UW Madison
- Lecturer at Lawrence University
- Lecturer at ISU

&lt;br&gt;

**Consulting**

- AES Statistical Consultant
- NREM Research Assistant
  
&lt;br&gt;

**Internship**

- Sandia National Labs: Statistical Sciences Research and Development Intern
  
]

---

# Overview of Talk

- Background and Overview of Thesis

&lt;br&gt;

- Detailed explanation of Chapter 1
  - Visual Diagnostics of a Model Explainer -- Tools for the Assessment of LIME Explanations

&lt;br&gt;

- Plan for Chapter 2
  - Explaining Random Forests using Clustering of Trees

&lt;br&gt;

- Plan for Chapter 3
  - Extensions of Neural Network Explanation Tools to Functional Data

&lt;br&gt;

- Timeline and Discussion Points

---

class: middle, center, inverse

# Background and Overview of Thesis

---

# Explainable Machine Learning

- **Machine learning models**
  - Good in prediction problems
  - Many are considered "black-boxes" since too complex to directly interpret

- **Explainable machine learning**

  - .red[**Goal**]: Explain predictions made by black-box models
  - .red[**Overview papers/books**]: .medium[Gilpin, Bau, Yuan, Bajwa, Specter, and Kagal [1]; Guidotti, Monreale, Ruggieri, Turini, Pedreschi, and Giannotti [2]; Ming [3]; Mohseni, Zarei, and Ragan [4]; Molnar [5]]

- **European General Data Protection Regulation** (GDPR) 
  - Implemented in 2018 includes a "right to explanation"
  - Goodman and Flaxman [6]: 
  
    &gt; "It is reasonable to suppose that any adequate explanation would, at a minimum, provide an account of how input features relate to predictions, allowing one to answer questions such as: Is the model more or less likely to recommend a loan if the applicant is a minority?"

---

# Explainability versus Interpretability

No accepted definitions for explainability and interpretability [1; 7; 5; 8; 9]

**My definitions**:
  - In regards to a model
  - Implicitly used by Rudin [10] and Ribeiro, Singh, and Guestrin [11]
  
.pull-left[
**Interpretability** is the ability to .blue[directly use model parameters] to understand the .red[mechanism of how the model makes predictions].

- Linear regression model
`$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\cdots+\hat{\beta}_px_p$$`
]

.pull-right[
**Explainability** is the ability to .blue[use the model in an indirect manner] to understand the .red[relationships in the data captured by the model].

- LIME: local interpretable model-agnostic explanations [11]

&lt;img src="figures/lime.png" width="55%" style="display: block; margin: auto;" /&gt;

]

---

# Model Agnostic Methods

.pull-left[
**General Model Visualizations**

.red[Strategies for understanding any model]

.large[
- Removing the blindfold [12]
]

&lt;br&gt;

**Global Methods**

.red[Explanation for model as a whole]

.large[
- Partial dependence plots [13] and extensions:
  - Interactive partial dependence plots [14]
  - Individual conditional expectation plots [15]
  - Accumulated local effect plots [16]
  - Feature interaction plots [17; 18; 19]
- Global feature importance plots [20; 21; 22]
- Global surrogate models [5]
]

]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/mds.png" alt="Model in data-space from Wickham, Cook, and Hofmann (2015)." width="100%" /&gt;
&lt;p class="caption"&gt;Model in data-space from Wickham, Cook, and Hofmann (2015).&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/pdp.png" alt="Partial dependence plot from Molnar (2019)." width="75%" /&gt;
&lt;p class="caption"&gt;Partial dependence plot from Molnar (2019).&lt;/p&gt;
&lt;/div&gt;
]

---

# Model Agnostic Methods

.pull-left[
**Local Methods**

.red[Explanation for an individual prediction]

.large[
- Individual conditional importance plots [22]
- LIME [11]
- Anchors (scoped rules) [23]
- Shapely values 
- SHAP [24]
- breakDown [25]
]

&lt;br&gt;

**Example Based**

.red[Explanations based on examples from the data]

.large[
- Counterfactual examples [26; 27; 28; 29]
- Adversarial examples [30; 31; 32; 33]
- Prototypes and criticisms 
- Influential instances [34]
]
]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/shapley.png" alt="Bar chart of shapley values from Molnar (2019)." width="75%" /&gt;
&lt;p class="caption"&gt;Bar chart of shapley values from Molnar (2019).&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/adversarial.png" alt="Adversarial example where one pixel change affects prediction from Su (2019)." width="65%" /&gt;
&lt;p class="caption"&gt;Adversarial example where one pixel change affects prediction from Su (2019).&lt;/p&gt;
&lt;/div&gt;
]
---

# Model Specific Methods

.pull-left[
**Random Forests**

.large[
- Random forest impurity based feature importance [35]
- Sectioned scatterplots [36]
- Trace plots of trees [36]
- Simplified model [37]
- Forest floor visualizations [38]
- Interactive visualizations [39; 40]
]

&lt;br&gt;

**Neural Networks**

.large[
- Extracting tree structures [41]
- Saliency maps [42]
- Feature visualization [43]
- Grand tours [44]
- Flows [45]
]
]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/rf-splits.png" alt="Sectioned scatterplot from Urbanek (2008)." width="60%" /&gt;
&lt;p class="caption"&gt;Sectioned scatterplot from Urbanek (2008).&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/saliency.png" alt="Saliency maps from Simonyan, Vedaldi, and Zisserman (2013)." width="60%" /&gt;
&lt;p class="caption"&gt;Saliency maps from Simonyan, Vedaldi, and Zisserman (2013).&lt;/p&gt;
&lt;/div&gt;
]

---

# .medium[Assessments of Explainable Machine Learning]

.pull-left[
**General**

.red[Argument against black box model explanations] by Rudin [10]:
  
  - .blue["Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead"]
  
  - Explanations may not:
      - be faithful to the original model
      - be detailed enough to understand the "black-box" model
      - make sense
  
  - Debunks accuracy and interpretability trade-off myth
]

.pull-right[
**Method Specific**

.red[Assessment of LIME] [46]:
  - How to choose a local region?

&lt;br&gt;

.red[Assessment of counterfactual examples] [46]:
  - Issues with unjustified counterfactual examples

&lt;br&gt;

.red[Assessment of saliency maps] [47]:
  - Transformation to input data affects saliency map but not model
]

---

# Overview of Dissertation Chapters

**Chapter 1: Visual Diagnostics for LIME**

- Discuss importance of assessing LIME
- Suggest the use of visualizations for assessment and provide example visualizations

&lt;br&gt;

**Chapter 2: Visualizations for Explaining Random Forests**

- Use clustering to identify key tree structures within the random forest
- Improve visualizations for use of trees as global surrogate models

&lt;br&gt;

**Chapter 3: Visualizations for Explaining Neural Networks**

- Project for my internship with Sandia National Labs
- Application with functional data
- Visualizations for the explaining and understanding the models

---

class: middle, center, inverse

# Chapter 1

## .center[Visual Diagnostics of a Model Explainer -- Tools for the Assessment of LIME Explanations]

---

# Motivation

.pull-left[
Hare, Hofmann, and Carriquiry [48]:
- Want to provide quantitative evidence for whether two bullets were fired from the same gun
- Use high definition scans of striations on bullet lands to extract "signatures" 
- Compute similarity features to compare two signatures
]

.pull-right[
&lt;img src="figures/gun.png" width="50%" /&gt;&lt;img src="figures/bullet.png" width="50%" /&gt;
]

&lt;img src="figures/striae.png" width="50%" /&gt;&lt;img src="figures/signatures.png" width="50%" /&gt;

---
 
# Motivation

Hare, Hofmann, and Carriquiry [48] approach: 
  - Random forest model
  - 9 signature similarity features
  - Returns a score for a comparison

&lt;br&gt;



&lt;img src="figures/bullet_pcp.png" width="100%" /&gt;

---

# Motivation

**Our Original Goal**: Provide explanations for specific signatures comparisons

**Attempt**: Applied LIME

**Result**: Unreasonable explanations (e.g., LIME explanation does not agree with random forest prediction)

**Example**: Known non-match

&lt;img src="slides_files/figure-html/unnamed-chunk-13-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Conceptual Depiction of LIME

.pull-left[LIME Ribeiro, Singh, and Guestrin [11]:

- .orange[**L**ocal]
- .blue[**I**nterpretable]
- .grey[**M**odel-agnostic]
- .red[**E**xplanations]
]

.pull-right[
**Concept**: For *one prediction of interest*
- .orange[Focus on a neighborhood around the prediction of interest]
- .blue[Use inherently interpretable model] 
    - "Explainer model"
- .grey[Understand the complex model] 
- .red[Capture relationship between complex model predictions and predictor variables]
]



![](slides_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

---

# Importance of Assessing LIME

.pull-left[

Additional layer of complexity:
  - Start with a complex model
  - LIME uses another model
  - End with two models to assess
]

.pull-right[
Questions raised:

- Explainer model a good approximation?
- Appropriate local region?
- Relationship linear in the local region?
- Which tuning parameters to use when applying LIME?
]

&lt;br&gt;

![](slides_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---

# Visualizations for Model Assessment

Claims about LIME in Ribeiro, Singh, and Guestrin [11]:

- .red[**Interpretability**]: Easy to interpret the explainer model

- .red[**Faithfulness**]: Explainer model captures relationship between complex model predictions and features (in local region)

- .red[**Linearity**]: Ridge regression assumes linear relationship between complex model predictions and features

- .red[**Localness**]: Explanations are local in regards to prediction of interest

&lt;br&gt;

We suggest visual diagnostics to assess these claims:

- .blue[**Diagnostics for individual explanations**]
- .blue[**Diagnostics for sets of explanations**]
- .blue[**Diagnostics for comparisons of tuning parameters**]

**Note**: We focus on binary response variable and continuous predictor variables

---

# Sine Example Data



.pull-left[
.red[Training data]: 500 observations  
.red[Testing data]: 100 observations  
.red[Black-box model]: random forest
  
`\(x_1\sim Unif(\)` -10, 10 `\()\)`  
`\(x_2\sim Unif(\)` -10, 10 `\()\)`  
`\(x_3\sim \mbox{N}(0,1)\)`
]

.pull-right[
`$$y=\begin{cases} \mbox{blue} &amp; \mbox{ if } x^*_2 &gt; \sin\left(x^*_1\right) \\
  \mbox{red} &amp; \mbox{ if } x^*_2 \le \sin\left(x^*_1\right) \ . %\nolabel
  \end{cases}$$`
where  
`\(x^*_1=x_1\cos(\theta)-x_2\sin(\theta)\)`   
`\(x^*_2=x_1\sin(\theta)+x_2\cos(\theta)\)`   
`\(\theta=-0.9\)`
]

![](slides_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---

class: inverse-sub, middle, center

# Set 1 of Visualizations
## .center[Diagnostics for Individual Explanations]

---

# Diagnostics for Individual Explanations

**LIME Application**

.pull-left[
- .red[Explanation Set]: Prediction of interest
- .red[Implementation]: *lime* R package [49]
- .red[Number Explanation Features]: 2
- .red[Tuning Parameters]: Default
]

.pull-right[
- .red[Complex Model]: Random forest
- .red[Explainer Model]: Ridge regression
]



&lt;img src="slides_files/figure-html/unnamed-chunk-20-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 1a**: Data Simulation

- .red[Sample Size]: 4999 observations 
- .red[Sample Method]: Uniformly from 4 quantile bins for each training data feature

**Corresponding Diagnostic:**  
*Training Data Plot*

- .red[Axes]: two features selected by LIME
- .red[Points]: training data 
- .red[Color]: observed response
- .red[Lines]: 4 quantile bins boundaries
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 1b**: Complex Model Predictions
  
- .red[Model]: Complex model 
- .red[Data]: Simulated data

**Corresponding Diagnostic:** *Untransformed Simulated Data Plot*

- .red[Axes]: simulated data features
  - denoted as `\(x'_1\)` and `\(x'_2\)` )
- .red[Points]: simulated data
- .red[Diamond]: prediction of interest
- .red[Color]: random forest prediction (for 'blue')
- .red[Lines]: 4 quantile bins boundaries

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
]
---

# Diagnostics for Individual Explanations

.left-desc[
**Step 1c**: Interpretability Transformation

- .red[Data]: Simulated data features
- .red[Transformation]: Indicator function
  - 1 = observation falls in same quantile bin as prediction of interest
  - 0 = o.w.
  
**Corresponding Diagnostic:** *Transformed Simulated Data Plot*

- .red[Axes]: Simulated data features
- .red[Points]: simulated data
- .red[Diamond]: prediction of interest
- .red[Color]: random forest prediction (for 'blue')
- .red[Rectangle Shades]: interpretability transformed feature regions
]

.right-plot[


![](slides_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;
]
---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2a**: Assign Weights

- .red[Method]: Proximity of untransformed observation to prediction of interest
- .red[Distance Metric]: Gower 

**Corresponding Diagnostic:**  
*Proximity Weights Plot*

- .red[Axes]: Simulated data features
- .red[Rectangle Color]: average weight within hexagon region
- .red[Lines]: interpretability transformation boundaries
- .red[Diamond]: prediction of interest

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;
]
---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2b**: Feature Selection

- .red[Model]: Ridge regression
  - .red[Response]: complex model predictions
  - .red[Features]: interpretability transformed features
  - .red[Weights]: proximity weights
- Forward selection (if less than 6 features specified)
  
**Corresponding Diagnostic:**  
*Feature Selection Comparison Plot*

- .red[Axes]: Training data features versus feature selection method
- .red[Tile Color]: indicates if feature selected
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2c**: Fit Explainer Model

- .red[Model]: Ridge regression
  - .red[Response]: complex model predictions
  - .red[Features]: selected interpretability transformed features
  - .red[Weights]: proximity weights

**Corresponding Diagnostic:**  
*Explainer Model Prediction Plot*

- .red[Axes]: simulated data features
- .red[Diamond]: prediction of interest 
- .red[Rectangle]: Interpretability transformed regions
- .red[Color]: explainer model prediction

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2c**: Fit Explainer Model (continued)

**Corresponding Diagnostic:**  
*Explainer Model Residual Plot*

- Residual plot for the explainer model
- Points have been jittered in the x-direction
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 3a**: Explainer Model Interpretation

- .red[Focus]: Model coefficients
- .red[Interpretation]: Indicates support for/against classification

**Corresponding Diagnostic:** *Explanation Plot*

- Adaptation of plot from *lime* R package
- .red[Axes]: Explainer model (interpretability transformed) feature versus explainer model coefficient
- .red[Bar Length]: Magnitude of explainer model coefficients
- .red[Bar Color]: Value of the explainer model coefficients
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations


.left-desc[
**Step 3b**: Explainer Model Interpretation (continued)

**Corresponding Diagnostic:** *Complex and Explainer Model Comparison Plot*

- .red[Axes]: simulated data features
- .red[Points]: simulated data 
- .red[Diamond]: prediction of interest
- .red[Point Color]: random forest prediction (for 'blue') 
- .red[Point Size]: proximity weight
- .red[Lines]: interpretability transformation boundaries
- .red[Line Color]: explainer coefficient supports a random forest prediction of 'blue' (blue) or not (red)
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;
]

---

class: inverse-sub, middle, center

# Set 2 of Visualizations

## .center[Diagnostics for Sets of Explanations]

---

# Diagnostics for Sets of Explanations

**LIME Application**

.pull-left[
- .red[Explanation Set]: All test data observations
- .red[Implementation]: *lime* R package [49]
- .red[Number Explanation Features]: 2
- .red[Tuning Parameters]: Default
]

.pull-right[
- .red[Complex Model]: Random forest
- .red[Explainer Model]: Ridge regression
]

&lt;img src="slides_files/figure-html/unnamed-chunk-31-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Diagnostics for Sets of Explanations

**Explanation Set Plot**

Provides an overview of groupings of LIME explanations

- Adaptation to the *lime* plot of all explanations
- .red[Y-axis]: Interpretability transformed features
- .red[X-axis]: Observation in the data set
- .red[Tile Color]: Ridge regression coefficient from the corresponding model

![](slides_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;

---

# Diagnostics for Sets of Explanations

**Average Coefficient Plots**

Provides a summary of explainer model coefficients across the set of explanations within quantile bin regions

- .red[Axes]: Test data features
- .red[Cells]: Intersections of quantile bins
- .red[Color]: Average of ridge regression coefficients (or sum of ridge regression coefficients) for observations within a cell

![](slides_files/figure-html/unnamed-chunk-33-1.png)&lt;!-- --&gt;

---

# Diagnostics for Sets of Explanations

.left-desc[
**Explainer Prediction Distribution Plot**

Shows the relationship between the explainer model predictions and the quantile bins

- .red[Axes]: Test data features
- .red[Points]: Test data observations
- .red[Color]: Explainer model prediction
- .red[Lines]: Quantile bin boundaries
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Sets of Explanations

.left-desc[
**Prediction Comparison Plot**

Shows the relationship between the explainer model and complex model predictions

- .red[Y-Axis]: Explainer model predictions
- .red[X-Axis]: Complex model predictions
- .red[Points]: Observations from the test data
- .red[Color]: Corresponding explainer model `\(R^2\)`
- .red[Dashed Line]: 1-1 line
- .red[Solid Line]: Loess smoother fit to the points
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Sets of Explanations

.left-desc[
**Overlaid Prediction Comparison Plot**

Shows the relationship between the average explainer model predictions within a quantile bins cell and the complex model predictions

- .red[Axes]: Test data features
- .red[Points]: Test data observations
- .red[Point Color]: Explainer model prediction
- .red[Cells]: Intersections of quantile bins
- .red[Cell Color]: Average explainer model prediction
- .red[Black Circles]: Identify observations misclassified by the complex model
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-36-1.png)&lt;!-- --&gt;

]

---

class: inverse-sub, middle, center

# Set 3 of Visualizations

## .center[Diagnostics for Comparisons of Tuning Parameters]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

**LIME Application**

.pull-left[
- .red[Explanation Set]: All test data observations
- .red[Implementation]: *lime* R package [49]
- .red[Number Explanation Features]: 2
- .red[Tuning Parameters]:
  - 2 to 6 quantile bins 
  - Kernel density simulation
]

.pull-right[
- .red[Complex Model]: Random forest
- .red[Explainer Model]: Ridge regression
]


&lt;img src="slides_files/figure-html/unnamed-chunk-37-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

.left-desc[
**Feature Heatmap Plot**

Provides an overview of the features selected by LIME across observations and tuning parameters

- .red[Y-Axis]: Test data observation
- .red[X-Axis]: Data simulation method
- .red[Y-Facet]: LIME feature order selection (first, second, etc.)
- .red[X-Facet]: Density based or bin based simulation method
- .red[Color]: Feature selected

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;
]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

**Notation for Assessment Metrics**

.pull-left[
.red[**Data and Complex Model**]

For a set of `\(E\)` explanations

- `\(\textbf{X}\)` = matrix of observed data to be explained with `\(K\)` features and `\(E\)` observations
- `\(x_e\)` = observed feature vector for observation `\(e\)`
- `\(f\)` = complex model
- `\(f(x_e)\)` = complex model prediction for observation `\(e\)`
]

.pull-right[
.red[**Simulated and Transformed Data**]

For `\(x_e\)` and set of tuning parameters `\(t\)`

- `\(\textbf{X}_{e,t}'\)` = LIME simulated dataset with `\(K\)` features and `\(S\)` observations
- `\(x'_{e,t,s}\)` = feature vector for simulated data observation `\(s\)`
- `\(\textbf{Z}'_{e,t}\)` = matrix of interpretability transformed simulated data with `\(K\)` features and `\(S\)` observations
- `\(z'_{e,t,s}\)` = interpretability transformed feature vector
- `\(z_{e,t}\)` = interpretability transformed `\(x_e\)`
]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

**Notation for Assessment Metrics**

.pull-left[
.red[**Weights and Explainer Model**]

For `\(x_e\)` and set of tuning parameters `\(t\)`

- `\(\omega_t\)` = proximity distance metric
- `\(\omega_t\left(x_e, x'_{e,t,s}\right)\)` = weight assigned to `\(x'_{e,t,s}\)` which are the proximity between `\(x_e\)` and `\(x'_{e,t,s}\)`
- `\(g_{e,t}\)` = explainer model
- `\(g_{e,t}(z'_{e,s,t})\)` = explainer model prediction for interpretability transformed simulated data observation `\(s\)`
- `\(R_{e,t}^2\)` = `\(R^2\)` for `\(g_{e,t}\)`

]

.pull-right[

.red[**Summary of Indices**]

- `\(e\)` = explanation `\((e=1,...,E)\)`
- `\(t\)` = set of tuning parameters `\((t=1,...,T)\)`
- `\(s\)` = simulated observation `\((s=1,...,S)\)`
]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

**Assessment Metrics**

Over set of explanations; same set of tuning parameters:

- .red[**Average R Squared**]: Average of explainer model `\(R^2\)` values
`$$R^2_{\mbox{ave}} = \frac{1}{E}\sum_{e=1}^E R_{e,t}^2$$`

- .red[**Average Fidelity**]: Average weighted distance between explainer and complex model predictions for simulated data (based on metric introduced in Ribeiro, Singh, and Guestrin [11])

`$$\mathcal{L}_{\mbox{ave}} \ = \ \frac{1}{E}\sum_{e=1}^E\mathcal{L}(f, \ g_{e,t}, \ \pi_{t}) \ =  \ \frac{1}{E}\sum_{e=1}^E\sum_{s=1}^{S}\omega_{t}\left(x_e, x'_{e,t,s}\right)\left(f\left(x'_{e,t,s}\right)-g_{e,t}\left(z_{e,t,s}'\right)\right)^2$$`


- .red[**Mean Squared Explanation Error (MSEE)**]: MSE for comparing complex to explainer predictions for explanations

`$$MSEE=\frac{1}{E}\sum_{e=1}^E\left(f\left(x_e\right)-g_{e,t}\left(z_{e,t}\right)\right)^2$$`

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

.left-desc[
**Assessment Metric Plot**

Visualization of assessment metrics for comparing tuning parameter performance

- .red[Y-Axis]: Metric value
- .red[X-Axis]: Data simulation method
- .red[Y-Facet]: Metric type
- .red[X-Facet]: Density based or bin based simulation method
- .red[Point]: Represents application of LIME to a set of observations
- .red[Color]: Rank of the application based on metric value (within a metric)

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-39-1.png)&lt;!-- --&gt;
]

---

class: inverse-sub, middle, center

# Implementation and Application 

---

# R package: limeaid

.left-desc[
Our R package for visual LIME diagnostics: 

- Available on GitHub 
  - github.com/goodekat/limeaid
- Functionality:
  - Applying LIME with multiple tuning parameters
  - Computing assessment metrics
  - Plots:
      - Complex and explainer model comparison plot
      - Feature Heatmap Plot
      - Assessment Metric Plot
      
&lt;img src="figures/limeaid.png" width="33%" style="display: block; margin: auto auto auto 0;" /&gt;
]

.right-plot[

```r
feature_heatmap(
  explanations = sine_lime_explain$explain,
  order_method = "PCA"
  )
```

![](slides_files/figure-html/unnamed-chunk-41-1.png)&lt;!-- --&gt;

]

---

# .medium[Application of Visual Diagnostics to Bullet Random Forest]

Bullet random forest prediction example from motivation with the complex and explainer model comparison plot

.pull-left[
![](slides_files/figure-html/unnamed-chunk-42-1.png)&lt;!-- --&gt;
]

.pull-right[
![](slides_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;

]

---

# .medium[Application of Visual Diagnostics to Bullet Random Forest]

Feature heatmap of LIME explanation from Hare, Hofmann, and Carriquiry [48] bullet comparison forest model 

![](slides_files/figure-html/unnamed-chunk-44-1.png)&lt;!-- --&gt;

---

class: middle, center, inverse

# Chapter 2

## .center[Explaining Random Forests using Clustering of Trees]

---

# Current Plan

.left-desc[
**Motivation**

- Still interested in gaining insights for the random forest from [48]
- Focus on visualizations of the random forest trees

**Ideas**

- Tree as a global explainer model
  - Plot global explainer on top of Urbanek [36]'s trace plot
  
- Clustering to identify key trees in random forest
  - Use distance metric to compare trees
]

.right-plot[
&lt;img src="figures/global-tree.png" width="50%" style="display: block; margin: auto;" /&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/rf-trace.png" alt="Trace plot from Urbanek (2008)." width="65%" /&gt;
&lt;p class="caption"&gt;Trace plot from Urbanek (2008).&lt;/p&gt;
&lt;/div&gt;
]

---

class: middle, center, inverse

# Chapter 3 

## .center[Extensions of Neural Network Explanation Tools to Functional Data]

---

# Application from Sandia National Labs

&lt;img src="./figures/fun-data.png" width="90%" style="display: block; margin: auto;" /&gt;

&lt;img src="./figures/sandia-note.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# Current Approach

### Feature Visualization

.pull-left[

**Concept**

- Focus on a "location" in the neural network
- Determine features that maximize the activation function 
- Identify example observation that triggers a part of the network

**Process**

1. Fit a model
2. Fix estimated parameter values
3. Determine values that maximized activation function at desired "location"
]

.pull-right[

&lt;br&gt;

&lt;img src="./figures/nn.png" width="70%" style="display: block; margin: auto;" /&gt;

&lt;br&gt;
&lt;br&gt;

.center[.small[Image source: https://en.wikipedia.org/wiki/Artificial_neural_network]]

]

---

# Feature Visualization

&lt;img src="./figures/fv-1hl.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Feature Visualization

&lt;img src="./figures/fv-3hl.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Ideas for Future Work  

Feature visualization adjustments
  - Visualize the functional principal components
  
Applications/extensions of other methods
  - Permutation feature importance, saliency maps, and partial dependence plots

Visualizations of the paths of an observation through the network
  - Example: flow [45]

&lt;img src="figures/flow.png" width="60%" style="display: block; margin: auto;" /&gt;

---

class: middle, center, inverse

# Timeline and Discussion Points

---

# Timeline for Completion 

**Summer 2020**

- Chapter 3: Work on content (Sandia internship)

&lt;br&gt;

**Fall 2020**

- Chapter 2: Work on content
- Chapter 3: Write up work

&lt;br&gt;

**Spring 2021**

- Chapter 2: Write up work
- Defend dissertation

---

# Publication of Chapter 1

How to divide up the material from chapter 1 for publication?  

Current ideas:

.pull-left[
**Paper 1: Survey paper on LIME**

- Explain LIME in a statistical context
- Use visualizations to help explain the procedure
- Use diagnostic visualizations to assess LIME
- Highlight issues with LIME
- Use iris and sine data
]

.pull-right[
**Paper 2: Diagnostic plots for LIME**

- Motivate assessment of LIME using the bullet matching data (example of high stakes decision using machine learning)
- Present diagnostic plots
- Demonstrate issues found with LIME explanations
]

---

class: inverse, center, middle

# Questions?

---

class: inverse, center, middle

# References

---

# References

.small[
[1] L. H. Gilpin, D. Bau, B. Z. Yuan, et al. "Explaining Explanations:
An Overview of Interpretability of Machine Learning".  (2018).

[2] R. Guidotti, A. Monreale, S. Ruggieri, et al. "A Survey Of Methods
For Explaining Black Box Models".  (2018).

[3] Y. Ming. "A Survey on Visualization for Explainable Classifiers".
PhD thesis. 2017.

[4] S. Mohseni, N. Zarei, and E. D. Ragan. "A Survey of Evaluation
Methods and Measures for Interpretable Machine Learning".  (2018).

[5] C. Molnar. _Interpretable Machine Learning_. 2019.

[6] B. Goodman and S. Flaxman. "European Union regulations on
algorithmic decision-making and a "right to explanation"".  (2016).

[7] Z. C. Lipton. "The Mythos of Model Interpretability".  (2016).

[8] G. Montavon, W. Samek, and K. Müller. "Methods for Interpreting and
Understanding Deep Neural Networks".  (2017).

[9] W. J. Murdoch, C. Singh, K. Kumbier, et al. "Interpretable machine
learning: definitions, methods, and applications". In: _arXiv_ (2019).
DOI:
[10.1073/pnas.1900654116](https://doi.org/10.1073%2Fpnas.1900654116).

[10] C. Rudin. "Stop Explaining Black Box Machine Learning Models for
High Stakes Decisions and Use Interpretable Models Instead".  (2018).

[11] M. T. Ribeiro, S. Singh, and C. Guestrin. ""Why Should I Trust
You?": Explaining the Predictions of Any Classifier". In: _Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, San Francisco, CA, USA, August 13-17, 2016_. 2016, pp.
1135-1144.

[12] H. Wickham, D. Cook, and H. Hofmann. "Visualizing statistical
models: Removing the blindfold". In: _Statistical Analysis and Data
Mining: The ASA Data Science Journal_ 8.4 (2015), pp. 203-225. ISSN:
1932-1872. DOI:
[10.1002/sam.11271](https://doi.org/10.1002%2Fsam.11271).

[13] J. H. Friedman. "Greedy function approximation: A gradient
boosting machine." In: _The Annals of Statistics_ 29.5 (2001). ISSN:
0090-5364. DOI:
[10.1214/aos/1013203451](https://doi.org/10.1214%2Faos%2F1013203451).

[14] J. Krause, A. Perer, and K. Ng. "Interacting with Predictions:
Visual Inspection of Black-box Machine Learning Models".  (2016), pp.
5686-5697. DOI:
[10.1145/2858036.2858529](https://doi.org/10.1145%2F2858036.2858529).

[15] A. Goldstein, A. Kapelner, J. Bleich, et al. "Peeking Inside the
Black Box: Visualizing Statistical Learning with Plots of Individual
Conditional Expectation".  (2013).

[16] D. W. Apley and J. Zhu. "Visualizing the Effects of Predictor
Variables in Black Box Supervised Learning Models".  (2016).

[17] J. H. Friedman and B. E. Popescu. "Predictive learning via rule
ensembles". In: _The Annals of Applied Statistics_ 2.3 (2008), pp.
916-954. ISSN: 1932-6157. DOI:
[10.1214/07-aoas148](https://doi.org/10.1214%2F07-aoas148).

[18] B. M. Greenwell, B. C. Boehmke, and A. J. McCarthy. "A Simple and
Effective Model-Based Variable Importance Measure".  (2018).

[19] G. Hooker. "Discovering additive structure in black box
functions".  (2004), p. 575. DOI:
[10.1145/1014052.1014122](https://doi.org/10.1145%2F1014052.1014122).

[20] A. Fisher, C. Rudin, and F. Dominici. "Model Class Reliance:
Variable Importance Measures for any Machine Learning Model Class, from
the “Rashomon” Perspective".  (2018).

[21] A. Altmann, L. Toloşi, O. Sander, et al. "Permutation importance:
a corrected feature importance measure". In: _Bioinformatics_ 26.10
(2010), pp. 1340-1347. ISSN: 1367-4803. DOI:
[10.1093/bioinformatics/btq134](https://doi.org/10.1093%2Fbioinformatics%2Fbtq134).

[22] G. Casalicchio, C. Molnar, and B. Bischl. "Visualizing the Feature
Importance for Black Box Models".  (2019), pp. 655-670. ISSN:
2190-5053. DOI:
[10.1007/978-3-030-10925-7\_40](https://doi.org/10.1007%2F978-3-030-10925-7%5C_40).
]

---

# References

.small[
[23] M. T. Ribeiro, S. Singh, and C. Guestrin. "Anchors: High-Precision
Model-Agnostic Explanations".  (2018).

[24] S. Lundberg and S. Lee. "A Unified Approach to Interpreting Model
Predictions".  (2017).

[25] M. Staniak and P. Biecek. "Explanations of model predictions with
live and breakDown packages". In: _arXiv_ 10.2 (2018), p. 395. DOI:
[10.32614/rj-2018-072](https://doi.org/10.32614%2Frj-2018-072).

[26] S. Wachter, B. Mittelstadt, and C. Russell. "Counterfactual
Explanations without Opening the Black Box: Automated Decisions and the
GDPR".  (2017).

[27] D. Martens and F. Provost. "Explaining Data-Driven Document
Classifications". In: _MIS Quarterly_ 38.1 (2014), pp. 73-99. ISSN:
0276-7783. DOI:
[10.25300/misq/2014/38.1.04](https://doi.org/10.25300%2Fmisq%2F2014%2F38.1.04).

[28] A. V. Looveren and J. Klaise. "Interpretable Counterfactual
Explanations Guided by Prototypes". In: _arXiv_ (2019).

[29] T. Laugel, M. Lesot, C. Marsala, et al. "Inverse Classification
for Comparison-based Interpretability in Machine Learning". In: _arXiv_
(2017).

[30] C. Szegedy, W. Zaremba, I. Sutskever, et al. "Intriguing
properties of neural networks".  (2013).

[31] I. J. Goodfellow, J. Shlens, and C. Szegedy. "Explaining and
Harnessing Adversarial Examples". In: _arXiv_ (2014).

[32] B. Biggio and F. Roli. "Wild Patterns: Ten Years After the Rise of
Adversarial Machine Learning". In: _Pattern Recognition_ 84 (2018), pp.
317-331. ISSN: 0031-3203. DOI:
[10.1016/j.patcog.2018.07.023](https://doi.org/10.1016%2Fj.patcog.2018.07.023).

[33] J. Su, D. V. Vargas, and K. Sakurai. "One Pixel Attack for Fooling
Deep Neural Networks". In: _IEEE Transactions on Evolutionary
Computation_ 23.5 (2019), pp. 828-841. ISSN: 1089-778X. DOI:
[10.1109/tevc.2019.2890858](https://doi.org/10.1109%2Ftevc.2019.2890858).

[34] P. W. Koh and P. Liang. "Understanding Black-box Predictions via
Influence Functions". In: _arXiv_ (2017).

[35] L. Breiman. "Random Forests". In: _Machine Learning_ 45.1 (2001),
pp. 5-32. ISSN: 0885-6125. DOI:
[10.1023/a:1010933404324](https://doi.org/10.1023%2Fa%3A1010933404324).

[36] S. Urbanek. "Visualizing Trees and Forests". In: _Handbook of data
visualization_. Ed. by A. U. Chun-houh Chen Wolfgang Härdle. 2008, pp.
243-266. ISBN: 9783540330363.

[37] S. Hara and K. Hayashi. "Making Tree Ensembles Interpretable".
(2016).

[38] S. H. Welling, H. H. F. Refsgaard, P. B. Brockhoff, et al. "Forest
Floor Visualizations of Random Forests".  (2016).

[39] C. Beckett. "Rfviz: An Interactive Visualization Package for
Random Forests in R". In: _DigitalCommons@USU All Graduate Plan B and
otherReports_ (2018).

[40] N. da Silva, D. Cook, and E. Lee. "Interactive Graphics for
Visually Diagnosing Forest Classifiers in R".  (2017).

[41] M. W. Craven and J. W. Shavlik. "Extracting Tree-Structured
Representations of Trained Networks". In: _Advances in Neural
Information Processing Systems_ 8 (1996).

[42] K. Simonyan, A. Vedaldi, and A. Zisserman. "Deep Inside
Convolutional Networks: Visualising Image Classification Models and
Saliency Maps".  (2013).

[43] C. Olah, A. Mordvintsev, and L. Schubert. "Feature Visualization".
In: _Distill_ (2017). https://distill.pub/2017/feature-visualization.
DOI:
[10.23915/distill.00007](https://doi.org/10.23915%2Fdistill.00007).

[44] M. Li, Z. Zhao, and C. Scheidegger. "Visualizing Neural Networks
with the Grand Tour". In: _Distill_ (2020).
https://distill.pub/2020/grand-tour. DOI:
[10.23915/distill.00025](https://doi.org/10.23915%2Fdistill.00025).
]

---

# References

.small[
[45] A. Halnaut, R. Giot, R. Bourqui, et al. "Deep Dive into Deep
Neural Networks with Flows".  (2020), pp. 231-239. DOI:
[10.5220/0008989702310239](https://doi.org/10.5220%2F0008989702310239).

[46] T. Laugel, X. Renard, M. Lesot, et al. "Defining Locality for
Surrogates in Post-hoc Interpretablity". In: _CoRR_ abs/1806.07498
(2018). URL:
[http://arxiv.org/abs/1806.07498](http://arxiv.org/abs/1806.07498).

[47] P. Kindermans, S. Hooker, J. Adebayo, et al. "The (Un)reliability
of saliency methods".  (2017).

[48] E. Hare, H. Hofmann, and A. Carriquiry. "Automatic Matching of
Bullet Lands". In: _Annals of Applied Statistics_ (Jan. 2016). DOI:
[http://adsabs.harvard.edu/abs/2016arXiv160105788H](https://doi.org/http%3A%2F%2Fadsabs.harvard.edu%2Fabs%2F2016arXiv160105788H).

[49] T. L. Pedersen and M. Benesty. _lime: Local Interpretable
Model-Agnostic Explanations_. https://lime.data-imaginist.com,
https://github.com/thomasp85/lime. 2020.
]

---

class: inverse, center, middle

# Appendix

---

# .medium[Training and Testing Data for Bullet Random Forest]

&lt;img src="figures/bullet_pcp.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# .medium[Extensions to Diagnostics for Individual Explanations]

Visualizations with more features

![](slides_files/figure-html/unnamed-chunk-57-1.png)&lt;!-- --&gt;

![](slides_files/figure-html/unnamed-chunk-58-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Distribution of the response variable for the random forest features

![](slides_files/figure-html/unnamed-chunk-59-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Bivariate visualizations of the random forest training data

&lt;img src="slides_files/figure-html/unnamed-chunk-60-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

# .medium[Additional Bullet Application Plots]

Random forest variable importance plot

&lt;img src="slides_files/figure-html/unnamed-chunk-61-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# .medium[Additional Bullet Application Plots]

Assessment metric plot

![](slides_files/figure-html/unnamed-chunk-62-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Plots of LIME explanations for one case in the test data that is a known non-match



![](slides_files/figure-html/unnamed-chunk-64-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Plots of LIME explanations for one case in the test data that is a known match

![](slides_files/figure-html/unnamed-chunk-65-1.png)&lt;!-- --&gt;

---

# .medium[Feature Visualization Example Optimization Example]

- Response: `\(y\in\{0,1\}\)`
- Features: `$$PC = (PC_1, PC_2, PC_3, PC_4, PC_5)$$`
- Model: 1 hidden layer, 5 neurons
- Neuron `\(i\)` coefficients `\((i=1,...,5)\)`: `$$\left(\beta_{0,i}, \beta_{1,i}, \beta_{2,i}, \beta_{3,i}, \beta_{4,i}, \beta_{5,i}\right)$$`
- Activation Function: logistic `$$\sigma(v)=\frac{1}{1+e^{-v}}$$`
- Feature visualization optimization:

`$$\arg\underset{PC}{\max} \ \sigma\left(\hat{\beta}_{0,i} + \hat{\beta}_{1,i}PC_1 +\cdots+\hat{\beta}_{5,i}PC_5\right)$$`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
