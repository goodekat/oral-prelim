<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Visualization Methods for Explainable Machine Learning   Statistics PhD Oral Prelim</title>
    <meta charset="utf-8" />
    <meta name="author" content="Katherine Goode" />
    <meta name="date" content="2020-05-06" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <strong>Visualization Methods for Explainable Machine Learning</strong> <br> Statistics PhD Oral Prelim
### Katherine Goode
### Iowa State University
### May 6, 2020

---


&lt;style&gt;

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #7C2529;
  font-size: 20px;
  font-weight: 300;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.inverse {
  background-color: #6E6259;
  border-top: 80px solid #6E6259;
  text-shadow: none;
}

.inverse-sub {
  background-color: #707372;
  border-top: 80px solid #707372;
  text-shadow: none;
}
.inverse-sub h1, .inverse-sub h2, .inverse-sub h3 {
  color: #f3f3f3;
}

.remark-slide-content &gt; h1 {
  font-family: 'Songti SC';
  font-weight: normal;
  font-size: 45px;
  margin-top: -95px;
  margin-left: -00px;
  color: #FFFFFF;
}

.title-slide {
  background-color: #FFFFFF;
  border-top: 80px solid #7C2529;
  border-bottom: 20px solid #6E6259;
}

.title-slide &gt; h1  {
  color: #1A292C;
  font-size: 40px;
  text-shadow: none;
  font-weight: 400;
  text-align: left;
  margin-left: 15px;
  padding-top: 80px;
}

body {
  font-family: 'Songti SC';
}

.remark-slide-number {
  font-size: 13pt;
  font-family: 'Songti SC';
  color: #272822;
  opacity: 1;
}

.inverse .remark-slide-number {
  font-size: 13pt;
  font-family: 'Songti SC';
  color: #FAFAFA;
  opacity: 1;
}

.inverse-sub .remark-slide-number {
  font-size: 13pt;
  font-family: 'Songti SC';
  color: #FAFAFA;
  opacity: 1;
}

a, a &gt; code {
  color: #000000; /*default: rgb(249, 38, 114); || sets color of links */
  text-decoration: none; /* turns off background coloring of links */
}

.left-desc {
  width: 40%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 59%;
  float: right;
  padding-left: 1%;
}

.tiny{font-size: 30%}
.small{font-size: 50%}
.medium{font-size: 75%}
.red{color: #9A3324}
.blue{color: #006BA6}
.tan{color: #9B945F}
.grey{color: #707372}
.orange{color: #FAAA72}
.darkgrey{color: }
&lt;/style&gt;



# Personal Background

.pull-left[
**Education**

B.A. in Mathematics
  - Lawrence University (Appleton, WI)
  - Graduated in June 2013
  
M.S. in Statistics 
  - University of Wisconsin, Madison
  - Graduated in May 2015
  
Ph.D. in Statistics (in progress)
  - Iowa State University
  - Started in January 2016
]

.pull-right[

**Teaching**

- Teaching assistant at UW Madison
- Lecturer at Lawrence University
- Lecturer at ISU

**Consulting**

- AES Statistical Consultant
- NREM Research Assistant
  
**Internship**

- Sandia National Labs: Statistical Sciences Research and Development Intern
  
]

---

# Overview of Talk

1. Background and Overview of Thesis

2. Detailed explanation of Chapter 1
  - Visual Diagnostics of a Model Explainer -- Tools for the Assessment of LIME Explanations

3. Plan for Chapter 2
  - Explaining Random Forests using Clustering of Trees

4. Plan for Chapter 3
  - Extensions of Neural Network Explanation Tools to Functional Data

5. Timeline and Discussion Points

---

class: middle, center, inverse

# Background and Overview of Thesis

---

# Explainable Machine Learning

- **Machine learning models**
  - Good in prediction problems
  - Many considered "black-boxes" since too complex to directly interpret

- **Explainable machine learning**

  - .red[**Goal**]: Explain predictions made by black-box models
  - .red[**Overview papers/books**]: .medium[Gilpin, Bau, Yuan, Bajwa, Specter, and Kagal (2018); Guidotti, Monreale, Ruggieri, Turini, Pedreschi, and Giannotti (2018); Ming (2017); Mohseni, Zarei, and Ragan (2018); Molnar (2019)]

- **European General Data Protection Regulation** (GDPR) 
  - Implemented in 2018 includes a "right to explanation"
  - Goodman and Flaxman (2016): 
  
    &gt; "It is reasonable to suppose that any adequate explanation would, at a minimum, provide an account of how input features relate to predictions, allowing one to answer questions such as: Is the model more or less likely to recommend a loan if the applicant is a minority?"

---

# Explainability versus Interpretability

No accepted definitions for explainability and interpretability 
  - .medium[Gilpin, Bau, Yuan, et al. (2018); Lipton (2016); Molnar (2019); Montavon, Samek, and Müller (2017); Murdoch, Singh, Kumbier, Abbasi-Asl, and Yu (2019)]  

My definitions (implicitly used by Rudin (2018) and Ribeiro, Singh, and Guestrin (2016)):

.pull-left[
**Interpretability** is the ability to .blue[directly use model parameters] to understand the .red[mechanism of how the model makes predictions].

- Linear regression model
`$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\cdots+\hat{\beta}_px_p$$`
]

.pull-right[
**Explainability** is the ability to .blue[use the model in an indirect manner] to understand the .red[relationships in the data captured by the model].

- LIME: local interpretable model-agnostic explanations (Ribeiro, Singh, and Guestrin, 2016)

&lt;img src="figures/lime.png" width="50%" style="display: block; margin: auto;" /&gt;

]

---

# Model Agnostic Methods

.pull-left[
**General Model Visualizations**: .red[Strategies for understanding any model]

.medium[
- Removing the blindfold .small[(Wickham, Cook, and Hofmann, 2015)]
]

**Global Methods**: .red[Explanation for model as a whole]

.medium[
- Partial dependence plots .small[(Friedman, 2001)] and extensions:
  - Interactive partial dependence plots .small[(Krause, Perer, and Ng, 2016)]
  - Individual conditional expectation plots .small[(Goldstein, Kapelner, Bleich, and Pitkin, 2013)]
  - Accumulated local effect plots .small[(Apley and Zhu, 2016)]
  - Feature interaction plots .small[(Friedman and Popescu, 2008; Greenwell, Boehmke, and McCarthy, 2018; Hooker, 2004)]
- Global feature importance plots .small[(Fisher, Rudin, and Dominici, 2018; Altmann, Toloşi, Sander, and Lengauer, 2010; Casalicchio, Molnar, and Bischl, 2019)]
- Global surrogate models .small[(Molnar, 2019)]
]

]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/mds.png" alt="Model in data-space from Wickham, Cook, and Hofmann (2015)." width="100%" /&gt;
&lt;p class="caption"&gt;Model in data-space from Wickham, Cook, and Hofmann (2015).&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/pdp.png" alt="Partial dependence plot from Molnar (2019)." width="75%" /&gt;
&lt;p class="caption"&gt;Partial dependence plot from Molnar (2019).&lt;/p&gt;
&lt;/div&gt;
]

---

# Model Agnostic Methods

.pull-left[
**Local Methods**: .red[Explanation for an individual prediction]

.medium[
- Individual conditional importance plots .small[(Casalicchio, Molnar, and Bischl, 2019)]
- LIME .small[(Ribeiro, Singh, and Guestrin, 2016)]
- Anchors (scoped rules) .small[(Ribeiro, Singh, and Guestrin, 2018)]
- Shapely values .small[]
- SHAP .small[(Lundberg and Lee, 2017)]
- breakDown .small[(Staniak and Biecek, 2018)]
]

**Example Based**: .red[Explanations based on examples from the data]

.medium[
- Counterfactual examples .small[(Wachter, Mittelstadt, and Russell, 2017; Martens and Provost, 2014; Looveren and Klaise, 2019; Laugel, Lesot, Marsala, Renard, and Detyniecki, 2017)]
- Adversarial examples .small[(Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus, 2013; Goodfellow, Shlens, and Szegedy, 2014; Biggio and Roli, 2018; Su, Vargas, and Sakurai, 2019)]
- Prototypes and criticisms .small[]
- Influential instances .small[(Koh and Liang, 2017)]
]
]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/shapley.png" alt="Bar chart of shapley values from Molnar (2019)." width="75%" /&gt;
&lt;p class="caption"&gt;Bar chart of shapley values from Molnar (2019).&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/adversarial.png" alt="Adversarial example where one pixel change affects prediction from Su (2019)." width="65%" /&gt;
&lt;p class="caption"&gt;Adversarial example where one pixel change affects prediction from Su (2019).&lt;/p&gt;
&lt;/div&gt;
]
---

# Model Specific Methods

.pull-left[
**Random Forests**

.medium[
- Random forest impurity based feature importance .small[(Breiman, 2001)]
- Sectioned scatterplots .small[(Urbanek, 2008)]
- Trace plots of trees .small[(Urbanek, 2008)]
- Simplified model .small[(Hara and Hayashi, 2016a)]
- Forest floor visualizations .small[(Welling, Refsgaard, Brockhoff, and Clemmensen, 2016)]
- Interactive visualizations .small[(Beckett, 2018; da
Silva, Cook, and Lee, 2017)]
]

**Neural Networks**

.medium[
- Extracting tree structures .small[(Craven and Shavlik, 1996)]
- Saliency maps .small[(Simonyan, Vedaldi, and Zisserman, 2013)]
- Feature visualization .small[(Olah, Mordvintsev, and Schubert, 2017)]
- Grand tours .small[(Li, Zhao, and Scheidegger, 2020)]
- Flows .small[(Halnaut, Giot, Bourqui, and Auber, 2020)]
]
]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/rf-splits.png" alt="Sectioned scatterplot from Urbanek (2008)." width="60%" /&gt;
&lt;p class="caption"&gt;Sectioned scatterplot from Urbanek (2008).&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./figures/saliency.png" alt="Saliency maps from Simonyan, Vedaldi, and Zisserman (2013)." width="60%" /&gt;
&lt;p class="caption"&gt;Saliency maps from Simonyan, Vedaldi, and Zisserman (2013).&lt;/p&gt;
&lt;/div&gt;
]

---

# .medium[Assessments of Explainable Machine Learning]

.pull-left[
**General**

.red[Argument against black box model explanations] (Rudin, 2018):
  - Title: ".blue[Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead]"
  - "Explanations must be wrong."
  - Explanations may not:
      - be faithful to the original model
      - be detailed enough to understand the "black-box" model
      - make sense
  - Use interpretable models for high-stakes decisions
  - Debunks accuracy and interpretability trade-off myth
]

.pull-right[
**Method Specific**

.red[Assessment of LIME] (Laugel, Renard, Lesot, Marsala, and Detyniecki, 2018):
  - How to choose a local region?
  
.red[Assessment of counterfactual examples] (Laugel, Renard, Lesot, et al., 2018):
  - Issues with unjustified counterfactual examples

.red[Assessment of saliency maps] (Kindermans, Hooker, Adebayo, Alber, Schütt, Dähne, Erhan, and Kim, 2017):
  - Transformation to input data affects saliency map but not model
]

---

# Overview of Dissertation Chapters

**Chapter 1: Visual Diagnostics for LIME**

- Discuss importance of assessing LIME
- Suggest the use of visualizations for assessment and provide example visualizations

&lt;br&gt;

**Chapter 2: Visualizations for Explaining Random Forests**

- Use clustering to identify key tree structures within the random forest
- Improve visualizations for use of trees as global surrogate models

&lt;br&gt;

**Chapter 3: Visualizations for Explaining Neural Networks**

- Project for my internship with Sandia National Labs
- Application with functional data
- Visualizations for the explaining and understanding the models

---

class: middle, center, inverse

# Chapter 1

## .center[Visual Diagnostics of a Model Explainer -- Tools for the Assessment of LIME Explanations]

---

# Motivation

.pull-left[
Hare, Hofmann, and Carriquiry (2016):
- Want to provide quantitative evidence for whether two bullets were fired from the same gun
- Use high definition scans of striations on bullet lands to extract "signatures" 
- Compute similarity features to compare two signatures
]

.pull-right[
&lt;img src="figures/gun.png" width="50%" /&gt;&lt;img src="figures/bullet.png" width="50%" /&gt;
]

&lt;img src="figures/striae.png" width="50%" /&gt;&lt;img src="figures/signatures.png" width="50%" /&gt;

---
 
# Motivation

Hare, Hofmann, and Carriquiry (2016) approach: 
  - Random forest model
  - 9 signature similarity features
  - Returns a score for the comparison of two lands

&lt;img src="figures/bullet_pcp.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Motivation

**Our Original Goal**: Provide explanations for specific signatures comparisons

**Attempt**: Applied LIME

**Result**: Unreasonable explanations (e.g., LIME explanation does not agree with random forest prediction)

**Example**: Known non-match

&lt;img src="slides_files/figure-html/unnamed-chunk-12-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Conceptual Depiction of LIME

.pull-left[LIME (Ribeiro, Singh, and Guestrin, 2016):

- .orange[**L**ocal]
- .blue[**I**nterpretable]
- .grey[**M**odel-agnostic]
- .red[**E**xplanations]
]

.pull-right[
**Concept**: For *one prediction of interest*
- .orange[Focus on a neighborhood around the prediction of interest]
- .blue[Use an inherently interpretable model]
- .grey[Understand the complex model] 
- .red[Capture the relationship between the complex model predictions and predictor variables]
]



![](slides_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

---

# Importance of Assessing LIME

.pull-left[

Additional layer of complexity:
  - Start with a complex model
  - LIME uses another model
  - End with two models to assess
]

.pull-right[
Questions raised:

- Explainer model a good approximation?
- Appropriate local region?
- Relationship linear in the local region?
- Which tuning parameter settings to use when applying LIME?
]

&lt;br&gt;

![](slides_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

---

# Visualizations for Model Assessment

Claims about LIME (Ribeiro, Singh, and Guestrin, 2016):

- .red[**Interpretability**]: Easy to interpret the explainer model

- .red[**Faithfulness**]: Explainer model captures relationship between complex model predictions and features (in local region)

- .red[**Linearity**]: Ridge regression assumes linear relationship between complex model predictions and features

- .red[**Localness**]: Explanations are local in regards to prediction of interest

&lt;br&gt;

We suggest visual diagnostics to assess these claims:

- .blue[**Diagnostics for individual explanations**]
- .blue[**Diagnostics for sets of explanations**]
- .blue[**Diagnostics for comparisons of tuning parameters**]

**Note**: We focus on binary response variable and continuous predictor variables

---

# Sine Example Data



.pull-left[
Training data: 500 observations  
Testing data: 100 observations  
Black-box model: random forest
  
`\(x_1\sim Unif(\)` -10, 10 `\()\)`  
`\(x_2\sim Unif(\)` -10, 10 `\()\)`  
`\(x_3\sim \mbox{N}(0,1)\)`
]

.pull-right[
`$$y=\begin{cases} \mbox{blue} &amp; \mbox{ if } x'_2 &gt; \sin\left(x'_1\right) \\
  \mbox{red} &amp; \mbox{ if } x'_2 \le \sin\left(x'_1\right) \ . %\nolabel
  \end{cases}$$`
where  
`\(x'_1=x_1\cos(\theta)-x_2\sin(\theta)\)`   
`\(x'_2=x_1\sin(\theta)+x_2\cos(\theta)\)`   
`\(\theta=-0.9\)`
]

![](slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---

class: inverse-sub, middle, center

# Set 1 of Visualizations
## .center[Diagnostics for Individual Explanations]

---

# Diagnostics for Individual Explanations

Applied LIME using *lime* R package (Pedersen and Benesty, 2020) 

- To prediction of interest
- Number of features to return in explanation: 2
- Default tuning parameters settings

*lime* R package visualization of the explanation:



&lt;img src="slides_files/figure-html/unnamed-chunk-19-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 1a**: Data Simulation

- Sample 4999 observations uniformly from 4 quantile bins for each feature in the training data 
  
**Corresponding Diagnostic:**  
*Training Data Plot*

- .red[Axes]: two features selected by LIME
- .red[Points]: training data 
- .red[Color]: observed response
- .red[Lines]: 4 quantile bins boundaries
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 1b**: Complex Model Predictions
  
- Apply complex model to simulated data to obtain predictions

**Corresponding Diagnostic:** *Untransformed Simulated Data Plot*

- .red[Axes]: simulated data features
  - denoted as `\(x'_1\)` and `\(x'_2\)` )
- .red[Points]: simulated data
- .red[Diamond]: prediction of interest
- .red[Color]: random forest prediction (for 'blue')
- .red[Lines]: 4 quantile bins boundaries

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;
]
---

# Diagnostics for Individual Explanations

.left-desc[
**Step 1c**: Interpretability Transformation

- Convert continuous features to binary variables based on whether the observation falls in the same quantile bin as the prediction of interest or not

**Corresponding Diagnostic:** *Transformed Simulated Data Plot*

- .red[Axes]: Simulated data features
- .red[Points]: simulated data
- .red[Diamond]: prediction of interest
- .red[Color]: random forest prediction (for 'blue')
- .red[Rectangle Shades]: interpretability transformed feature regions
]

.right-plot[


![](slides_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;
]
---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2a**: Assign Weights

- Assign weights to simulated data based on proximity to the prediction of interest (using untransformed feature values)
- Gower distance metric 

**Corresponding Diagnostic:**  
*Proximity Weights Plot*

- .red[Axes]: Simulated data features
- .red[Rectangle Color]: average weight within hexagon region
- .red[Lines]: interpretability transformation boundaries
- .red[Diamond]: prediction of interest

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;
]
---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2b**: Feature Selection

- Ridge regression model fit to simulated data
  - Response: complex model predictions
  - Features: interpretability transformed features
  - Weights: proximity weights
- Forward selection (if less than 6 features specified)
  
**Corresponding Diagnostic:**  
*Feature Selection Comparison Plot*

- .red[Axes]: Training data features versus feature selection method
- .red[Tile Color]: indicates if feature selected
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2c**: Fit Explainer Model

- Ridge regression model fit to simulated data
  - Response: complex model predictions
  - Features: selected interpretability transformed features
  - Weights: proximity weights

**Corresponding Diagnostic:**  
*Explainer Model Prediction Plot*

- .red[Axes]: simulated data features
- .red[Diamond]: prediction of interest 
- .red[Rectangle]: Interpretability transformed regions
- .red[Color]: explainer model prediction

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 2c**: Fit Explainer Model (continued)

**Corresponding Diagnostic:**  
*Explainer Model Residual Plot*

- Residual plot for the explainer model
- Points have been jittered in the x-direction
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations

.left-desc[
**Step 3a**: Explainer Model Interpretation

- Interpret coefficients of explainer model
- Explains complex model prediction

**Corresponding Diagnostic:** *Explanation Plot*

- Adaptation of plot from *lime* R package
- .red[Axes]: Explainer model (interpretability transformed) feature versus explainer model coefficient
- .red[Bar Length]: magnitude of explainer model coefficients
- .red[Bar Color]: value of the explainer model coefficients
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Individual Explanations


.left-desc[
**Step 3b**: Explainer Model Interpretation (continued)

**Corresponding Diagnostic:** *Complex and Explainer Model Comparison Plot*

- .red[Axes]: simulated data features
- .red[Points]: simulated data 
- .red[Diamond]: prediction of interest
- .red[Point Color]: random forest prediction (for 'blue') 
- .red[Point Size]: proximity weight
- .red[Lines]: interpretability transformation boundaries
- .red[Line Color]: explainer coefficient supports a random forest prediction of 'blue' (blue) or not (red)
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;
]

---

class: inverse-sub, middle, center

# Set 2 of Visualizations

## .center[Diagnostics for Sets of Explanations]

---

# Diagnostics for Sets of Explanations

Applied LIME using *lime* R package (Pedersen and Benesty, 2020) 

- All test data observations
- Number of features to return in explanation: 2
- Default tuning parameters settings

Plot of all explanations from *lime* R package: 

&lt;img src="slides_files/figure-html/unnamed-chunk-30-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Diagnostics for Sets of Explanations

**Explanation Set Plot**

Provides an overview of groupings of LIME explanations

- Adaptation to the *lime* plot of all explanations
- .red[Y-axis]: Interpretability transformed features
- .red[X-axis]: Observation in the data set
- .red[Tile Color]: Ridge regression coefficient from the corresponding model

![](slides_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;

---

# Diagnostics for Sets of Explanations

**Average Coefficient Plots**

Provides a summary of explainer model coefficients across the set of explanations within quantile bin regions

- .red[Axes]: Test data features
- .red[Cells]: Intersections of quantile bins
- .red[Color]: Average of ridge regression coefficients (or sum of ridge regression coefficients) for observations within a cell

![](slides_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;

---

# Diagnostics for Sets of Explanations

.left-desc[
**Explainer Prediction Distribution Plot**

Shows the relationship between the explainer model predictions and the quantile bins

- .red[Axes]: Test data features
- .red[Points]: Test data observations
- .red[Color]: Explainer model prediction
- .red[Lines]: Quantile bin boundaries
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-33-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Sets of Explanations

.left-desc[
**Prediction Comparison Plot**

Shows the relationship between the explainer model and complex model predictions

- .red[Y-Axis]: Explainer model predictions
- .red[X-Axis]: Complex model predictions
- .red[Points]: Observations from the test data
- .red[Color]: Corresponding explainer model `\(R^2\)`
- .red[Dashed Line]: 1-1 line
- .red[Solid Line]: Loess smoother fit to the points
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;
]

---

# Diagnostics for Sets of Explanations

.left-desc[
**Overlaid Prediction Comparison Plot**

Shows the relationship between the average explainer model predictions within a quantile bins cell and the complex model predictions

- .red[Axes]: Test data features
- .red[Points]: Test data observations
- .red[Point Color]: Explainer model prediction
- .red[Cells]: Intersections of quantile bins
- .red[Cell Color]: Average explainer model prediction
- .red[Black Circles]: Identify observations misclassified by the complex model
]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;

]

---

class: inverse-sub, middle, center

# Set 3 of Visualizations

## .center[Diagnostics for Comparisons of Tuning Parameters]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

Applied LIME using *lime* R package (Pedersen and Benesty, 2020)
- All test data observations  
- Number of features to return in explanation: 2
- Multiple applications of LIME for each observation
  - 2 to 6 quantile bins
  - Kernel density simulation

Plot of all explanations from *lime* R package: 

&lt;img src="slides_files/figure-html/unnamed-chunk-36-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

.left-desc[
**Feature Heatmap Plot**

Provides an overview of the features selected by LIME across observations and tuning parameters

- .red[Y-Axis]: Test data observation
- .red[X-Axis]: Data simulation method
- .red[Y-Facet]: LIME feature order selection (first, second, etc.)
- .red[X-Facet]: Density based or bin based simulation method
- .red[Color]: Feature selected

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-37-1.png)&lt;!-- --&gt;
]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

### Notation for Assessment Metrics

.pull-left[
.red[**Data and Complex Model**]

For a set of `\(E\)` explanations

- `\(\textbf{X}\)` = matrix of observed data to be explained with `\(K\)` features and `\(E\)` observations
- `\(x_e\)` = observed feature vector for observation `\(e\)`
- `\(f\)` = complex model
- `\(f(x_e)\)` = complex model prediction for observation `\(e\)`
]

.pull-right[
.red[**Simulated and Transformed Data**]

For `\(x_e\)` and set of tuning parameters `\(t\)`

- `\(\textbf{X}_{e,t}'\)` = LIME simulated dataset with `\(K\)` features and `\(S\)` observations
- `\(x'_{e,t,s}\)` = feature vector for simulated data observation `\(s\)`
- `\(\textbf{Z}'_{e,t}\)` = matrix of interpretability transformed simulated data with `\(K\)` features and `\(S\)` observations
- `\(z'_{e,t,s}\)` = interpretability transformed feature vector
- `\(z_{e,t}\)` = interpretability transformed `\(x_e\)`
]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

### Notation for Assessment Metrics

.pull-left[
.red[**Weights and Explainer Model**]

For `\(x_e\)` and set of tuning parameters `\(t\)`

- `\(\omega_t\)` = proximity distance metric
- `\(\omega_t\left(x_e, x'_{e,t,s}\right)\)` = weight assigned to `\(x'_{e,t,s}\)` which are the proximity between `\(x_e\)` and `\(x'_{e,t,s}\)`
- `\(g_{e,t}\)` = explainer model
- `\(g_{e,t}(z'_{e,s,t})\)` = explainer model prediction for interpretability transformed simulated data observation `\(s\)`

]

.pull-right[

.red[**Summary of Indices**]

- `\(e\)` = explanation `\((e=1,...,E)\)`
- `\(t\)` = set of tuning parameters `\((t=1,...,T)\)`
- `\(s\)` = simulated observation `\((s=1,...,S)\)`
]

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

### Metrics for the Assessment of LIME

.red[**Average R Squared**]: Average of explainer model `\(R^2\)` values over a set of explanations with the same tuning parameters to assess the linearity claim where `\(R_{e,t}^2\)` is the `\(R^2\)` for `\(g_{e,t}\)`
`$$R^2_{\mbox{ave}} = \frac{1}{E}\sum_{e=1}^E R_{e,t}^2$$`

.red[**Average Fidelity**]: Average of fidelity metric introduced in Ribeiro, Singh, and Guestrin (2016) over a set of explanations with the same tuning parameters to assess the explainer model approximation to the complex model

`$$\mathcal{L}_{\mbox{ave}} \ = \ \frac{1}{E}\sum_{e=1}^E\mathcal{L}(f, \ g_{e,t}, \ \pi_{t}) \ =  \ \frac{1}{E}\sum_{e=1}^E\sum_{s=1}^{S}\omega_{t}\left(x'_e, x'_{e,t,s}\right)\left(f\left(x'_{e,t,s}\right)-g_{e,t}\left(z_{e,t,s}'\right)\right)^2$$`


.red[**Mean Squared Explanation Error (MSEE)**]: Comparable to an MSE for comparing complex model predictions to explainer model predictions for each explanation

`$$MSEE=\frac{1}{E}\sum_{e=1}^E\left(f\left(x_e\right)-g_{e,t}\left(z_{e,t}\right)\right)^2$$`

---

# .medium[Diagnostics for Comparisons of Tuning Parameters]

.left-desc[
**Assessment Metric Plot**

Visualization of assessment metrics for comparing tuning parameter performance

- .red[Y-Axis]: Metric value
- .red[X-Axis]: Data simulation method
- .red[Y-Facet]: Metric type
- .red[X-Facet]: Density based or bin based simulation method
- .red[Point]: Represents application of LIME to a set of observations
- .red[Color]: Rank of the application based on metric value (within a metric)

]

.right-plot[
![](slides_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;
]

---

class: inverse-sub, middle, center

# Implementation and Application 

---

# R package: limeaid

.left-desc[
Our R package for visual LIME diagnostics: 

- Available on GitHub 
  - github.com/goodekat/limeaid
- Functionality:
  - Applying LIME with multiple tuning parameters
  - Computing assessment metrics
  - Plots:
      - Complex and explainer model comparison plot
      - Feature Heatmap Plot
      - Assessment Metric Plot
      
&lt;img src="figures/limeaid.png" width="33%" style="display: block; margin: auto auto auto 0;" /&gt;
]

.right-plot[

```r
feature_heatmap(
  explanations = sine_lime_explain$explain,
  order_method = "PCA"
  )
```

![](slides_files/figure-html/unnamed-chunk-40-1.png)&lt;!-- --&gt;

]

---

# .medium[Application of Visual Diagnostics to Bullet Random Forest]

Bullet random forest prediction example from motivation with the complex and explainer model comparison plot

.pull-left[
![](slides_files/figure-html/unnamed-chunk-41-1.png)&lt;!-- --&gt;
]

.pull-right[
![](slides_files/figure-html/unnamed-chunk-42-1.png)&lt;!-- --&gt;

]

---

# .medium[Application of Visual Diagnostics to Bullet Random Forest]

Feature heatmap of LIME explanation from Hare, Hofmann, and Carriquiry (2016) bullet comparison forest model 

![](slides_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;

---

class: middle, center, inverse

# Chapter 2

## .center[Explaining Random Forests using Clustering of Trees]

---

# Current Plan

.left-desc[
**Motivation**

- Still interested in gaining insights for the random forest from (Hare, Hofmann, and Carriquiry, 2016)
- Focus on visualizations of the random forest trees

**Ideas**

- Tree as a global explainer model
  - Plot global explainer on top of Urbanek (2008)'s trace plot
  
- Clustering to identify key trees in random forest
  - Use distance metric to compare trees
]

.right-plot[
&lt;img src="figures/global-tree.png" width="50%" style="display: block; margin: auto;" /&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/rf-trace.png" alt="Trace plot from Urbanek (2008)." width="65%" /&gt;
&lt;p class="caption"&gt;Trace plot from Urbanek (2008).&lt;/p&gt;
&lt;/div&gt;
]

---

class: middle, center, inverse

# Chapter 3 

## .center[Extensions of Neural Network Explanation Tools to Functional Data]

---

# Application from Sandia National Labs

&lt;img src="./figures/fun-data.png" width="90%" style="display: block; margin: auto;" /&gt;

&lt;img src="./figures/sandia-note.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# Current Approach

### Feature Visualization

.pull-left[

**Concept**

- Focus on a "location" in the neural network
- Determine features that maximize the activation function 
- Identify example observation that triggers a part of the network

**Process**

1. Fit a model
2. Fix estimated parameter values
3. Determine values that maximized activation function at desired "location"
]

.pull-right[

&lt;br&gt;

&lt;img src="./figures/nn.png" width="70%" style="display: block; margin: auto;" /&gt;

&lt;br&gt;
&lt;br&gt;

.center[.small[Image source: https://en.wikipedia.org/wiki/Artificial_neural_network]]

]

---

# Feature Visualization

&lt;img src="./figures/fv-1hl.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Feature Visualization

&lt;img src="./figures/fv-3hl.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Ideas for Future Work  

Feature visualization adjustments
  - Visualize the functional principal components
  
Applications/extensions of other methods
  - Permutation feature importance, saliency maps, and partial dependence plots

Visualizations of the paths of an observation through the network
  - Example: flow (Halnaut, Giot, Bourqui, et al., 2020)

&lt;img src="figures/flow.png" width="60%" style="display: block; margin: auto;" /&gt;

---

class: middle, center, inverse

# Timeline and Discussion Points

---

# Timeline for Completion 

**Summer 2020**

- Chapter 1: Finish writing (possibly submit paper)
- Chapter 2: Start on content (if time)
- .blue[Chapter 3: Work on content (Sandia internship)]

**Fall 2020**

- Chapter 1: Submit paper (if not already done)
- .blue[Chapter 2: Work on content]
- .blue[Chapter 3: Write up work]

**Spring 2021**

- Chapter 1: Review process (if applicable)
- .blue[Chapter 2: Write up work]
- Chapter 3: Submit paper (if ready)
- .blue[Defend dissertation]

---

# Publication of Chapter 1

How to divide up the material from chapter 1 for publication?  

Current ideas:

.pull-left[
**Paper 1: Survey paper on LIME**

- Explain LIME in a statistical context
- Use visualizations to help explain the procedure
- Use diagnostic visualizations to assess LIME
- Highlight issues with LIME
- Use iris and sine data
]

.pull-right[
**Paper 2: Diagnostic plots for LIME**

- Motivate assessment of LIME using the bullet matching data (example of high stakes decision using machine learning)
- Present diagnostic plots
- Demonstrate issues found with LIME explanations
]

---

class: inverse, center, middle

# References

---

# References

.small[
Altmann, A, L. Toloşi, O. Sander, et al. (2010). "Permutation
importance: a corrected feature importance measure". In:
_Bioinformatics_ 26.10, pp. 1340-1347. ISSN: 1367-4803. DOI:
[10.1093/bioinformatics/btq134](https://doi.org/10.1093%2Fbioinformatics%2Fbtq134).

Apley, D. W. and J. Zhu (2016). "Visualizing the Effects of Predictor
Variables in Black Box Supervised Learning Models".

Beckett, C. (2018). "Rfviz: An Interactive Visualization Package for
Random Forests in R". In: _DigitalCommons@USU All Graduate Plan B and
otherReports_.

Biggio, B. and F. Roli (2018). "Wild Patterns: Ten Years After the Rise
of Adversarial Machine Learning". In: _Pattern Recognition_ 84, pp.
317-331. ISSN: 0031-3203. DOI:
[10.1016/j.patcog.2018.07.023](https://doi.org/10.1016%2Fj.patcog.2018.07.023).

Breiman, L. (2001). "Random Forests". In: _Machine Learning_ 45.1, pp.
5-32. ISSN: 0885-6125. DOI:
[10.1023/a:1010933404324](https://doi.org/10.1023%2Fa%3A1010933404324).

Casalicchio, G, C. Molnar, and B. Bischl (2019). "Visualizing the
Feature Importance for Black Box Models" , pp. 655-670. ISSN:
2190-5053. DOI:
[10.1007/978-3-030-10925-7\_40](https://doi.org/10.1007%2F978-3-030-10925-7%5C_40).

Craven, M. W. and J. W. Shavlik (1996). "Extracting Tree-Structured
Representations of Trained Networks". In: _Advances in Neural
Information Processing Systems_ 8.

Fisher, A, C. Rudin, and F. Dominici (2018). "Model Class Reliance:
Variable Importance Measures for any Machine Learning Model Class, from
the “Rashomon” Perspective".

Friedman, J. H. (2001). "Greedy function approximation: A gradient
boosting machine." In: _The Annals of Statistics_ 29.5. ISSN:
0090-5364. DOI:
[10.1214/aos/1013203451](https://doi.org/10.1214%2Faos%2F1013203451).

Friedman, J. H. and B. E. Popescu (2008). "Predictive learning via rule
ensembles". In: _The Annals of Applied Statistics_ 2.3, pp. 916-954.
ISSN: 1932-6157. DOI:
[10.1214/07-aoas148](https://doi.org/10.1214%2F07-aoas148).

Gilpin, L. H, D. Bau, B. Z. Yuan, et al. (2018). "Explaining
Explanations: An Overview of Interpretability of Machine Learning".

Goldstein, A, A. Kapelner, J. Bleich, et al. (2013). "Peeking Inside
the Black Box: Visualizing Statistical Learning with Plots of
Individual Conditional Expectation".

Goodfellow, I. J, J. Shlens, and C. Szegedy (2014). "Explaining and
Harnessing Adversarial Examples". In: _arXiv_.

Goodman, B. and S. Flaxman (2016). "European Union regulations on
algorithmic decision-making and a "right to explanation"".

Greenwell, B. M, B. C. Boehmke, and A. J. McCarthy (2018). "A Simple
and Effective Model-Based Variable Importance Measure".

Guidotti, R, A. Monreale, S. Ruggieri, et al. (2018). "A Survey Of
Methods For Explaining Black Box Models".

Halnaut, A, R. Giot, R. Bourqui, et al. (2020). "Deep Dive into Deep
Neural Networks with Flows" , pp. 231-239. DOI:
[10.5220/0008989702310239](https://doi.org/10.5220%2F0008989702310239).

Hara, S. and K. Hayashi (2016a). "Making Tree Ensembles Interpretable".

Hare, E, H. Hofmann, and A. Carriquiry (2016). "Automatic Matching of
Bullet Lands". In: _Annals of Applied Statistics_. DOI:
[http://adsabs.harvard.edu/abs/2016arXiv160105788H](https://doi.org/http%3A%2F%2Fadsabs.harvard.edu%2Fabs%2F2016arXiv160105788H).

Hooker, G. (2004). "Discovering additive structure in black box
functions" , p. 575. DOI:
[10.1145/1014052.1014122](https://doi.org/10.1145%2F1014052.1014122).

Kindermans, P, S. Hooker, J. Adebayo, et al. (2017). "The
(Un)reliability of saliency methods".

Koh, P. W. and P. Liang (2017). "Understanding Black-box Predictions
via Influence Functions". In: _arXiv_.
]

---

# References

.small[
Krause, J, A. Perer, and K. Ng (2016). "Interacting with Predictions:
Visual Inspection of Black-box Machine Learning Models" , pp.
5686-5697. DOI:
[10.1145/2858036.2858529](https://doi.org/10.1145%2F2858036.2858529).

Laugel, T, M. Lesot, C. Marsala, et al. (2017). "Inverse Classification
for Comparison-based Interpretability in Machine Learning". In:
_arXiv_.

Laugel, T, X. Renard, M. Lesot, et al. (2018). "Defining Locality for
Surrogates in Post-hoc Interpretablity". In: _CoRR_ abs/1806.07498.
URL:
[http://arxiv.org/abs/1806.07498](http://arxiv.org/abs/1806.07498).

Li, M, Z. Zhao, and C. Scheidegger (2020). "Visualizing Neural Networks
with the Grand Tour". In: _Distill_.
https://distill.pub/2020/grand-tour. DOI:
[10.23915/distill.00025](https://doi.org/10.23915%2Fdistill.00025).

Lipton, Z. C. (2016). "The Mythos of Model Interpretability".

Looveren, A. V. and J. Klaise (2019). "Interpretable Counterfactual
Explanations Guided by Prototypes". In: _arXiv_.

Lundberg, S. and S. Lee (2017). "A Unified Approach to Interpreting
Model Predictions".

Martens, D. and F. Provost (2014). "Explaining Data-Driven Document
Classifications". In: _MIS Quarterly_ 38.1, pp. 73-99. ISSN: 0276-7783.
DOI:
[10.25300/misq/2014/38.1.04](https://doi.org/10.25300%2Fmisq%2F2014%2F38.1.04).

Ming, Y. (2017). "A Survey on Visualization for Explainable
Classifiers".

Mohseni, S, N. Zarei, and E. D. Ragan (2018). "A Survey of Evaluation
Methods and Measures for Interpretable Machine Learning".

Molnar, C. (2019). _Interpretable Machine Learning_.

Montavon, G, W. Samek, and K. Müller (2017). "Methods for Interpreting
and Understanding Deep Neural Networks".

Murdoch, W. J, C. Singh, K. Kumbier, et al. (2019). "Interpretable
machine learning: definitions, methods, and applications". In: _arXiv_.
DOI:
[10.1073/pnas.1900654116](https://doi.org/10.1073%2Fpnas.1900654116).

Olah, C, A. Mordvintsev, and L. Schubert (2017). "Feature
Visualization". In: _Distill_.
https://distill.pub/2017/feature-visualization. DOI:
[10.23915/distill.00007](https://doi.org/10.23915%2Fdistill.00007).

Pedersen, T. L. and M. Benesty (2020). _lime: Local Interpretable
Model-Agnostic Explanations_. https://lime.data-imaginist.com,
https://github.com/thomasp85/lime.

Ribeiro, M. T., S. Singh, and C. Guestrin (2016). ""Why Should I Trust
You?": Explaining the Predictions of Any Classifier". In: _Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, San Francisco, CA, USA, August 13-17, 2016_. , pp.
1135-1144.

Ribeiro, M. T, S. Singh, and C. Guestrin (2018). "Anchors:
High-Precision Model-Agnostic Explanations".

Rudin, C. (2018). "Stop Explaining Black Box Machine Learning Models
for High Stakes Decisions and Use Interpretable Models Instead".

Silva, N. da, D. Cook, and E. Lee (2017). "Interactive Graphics for
Visually Diagnosing Forest Classifiers in R".

Simonyan, K, A. Vedaldi, and A. Zisserman (2013). "Deep Inside
Convolutional Networks: Visualising Image Classification Models and
Saliency Maps".

Staniak, M. and P. Biecek (2018). "Explanations of model predictions
with live and breakDown packages". In: _arXiv_ 10.2, p. 395. DOI:
[10.32614/rj-2018-072](https://doi.org/10.32614%2Frj-2018-072).

Su, J, D. V. Vargas, and K. Sakurai (2019). "One Pixel Attack for
Fooling Deep Neural Networks". In: _IEEE Transactions on Evolutionary
Computation_ 23.5, pp. 828-841. ISSN: 1089-778X. DOI:
[10.1109/tevc.2019.2890858](https://doi.org/10.1109%2Ftevc.2019.2890858).
]

---

# References

.small[
Szegedy, C, W. Zaremba, I. Sutskever, et al. (2013). "Intriguing
properties of neural networks".

Urbanek, S. (2008). "Visualizing Trees and Forests". In: _Handbook of
data visualization_. Ed. by A. U. Chun-houh Chen Wolfgang Härdle , pp.
243-266. ISBN: 9783540330363.

Wachter, S, B. Mittelstadt, and C. Russell (2017). "Counterfactual
Explanations without Opening the Black Box: Automated Decisions and the
GDPR".

Welling, S. H, H. H. F. Refsgaard, P. B. Brockhoff, et al. (2016).
"Forest Floor Visualizations of Random Forests".

Wickham, H, D. Cook, and H. Hofmann (2015). "Visualizing statistical
models: Removing the blindfold". In: _Statistical Analysis and Data
Mining: The ASA Data Science Journal_ 8.4, pp. 203-225. ISSN:
1932-1872. DOI:
[10.1002/sam.11271](https://doi.org/10.1002%2Fsam.11271).
]

---

class: inverse, center, middle

# Appendix

---

# .medium[Extensions to Diagnostics for Individual Explanations]

Visualizations with more features

![](slides_files/figure-html/unnamed-chunk-55-1.png)&lt;!-- --&gt;

![](slides_files/figure-html/unnamed-chunk-56-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Distribution of the response variable for the random forest features

![](slides_files/figure-html/unnamed-chunk-57-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Bivariate visualizations of the random forest training data

&lt;img src="slides_files/figure-html/unnamed-chunk-58-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

# .medium[Additional Bullet Application Plots]

Random forest variable importance plot

&lt;img src="slides_files/figure-html/unnamed-chunk-59-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# .medium[Additional Bullet Application Plots]

Assessment metric plot

![](slides_files/figure-html/unnamed-chunk-60-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Plots of LIME explanations for one case in the test data that is a known non-match



![](slides_files/figure-html/unnamed-chunk-62-1.png)&lt;!-- --&gt;

---

# .medium[Additional Bullet Application Plots]

Plots of LIME explanations for one case in the test data that is a known match

![](slides_files/figure-html/unnamed-chunk-63-1.png)&lt;!-- --&gt;

---

# .medium[Feature Visualization Example Optimization Example]

- Response: `\(y\in\{0,1\}\)`
- Features: `$$PC = (PC_1, PC_2, PC_3, PC_4, PC_5)$$`
- Model: 1 hidden layer, 5 neurons
- Neuron `\(i\)` coefficients `\((i=1,...,5)\)`: `$$\left(\beta_{0,i}, \beta_{1,i}, \beta_{2,i}, \beta_{3,i}, \beta_{4,i}, \beta_{5,i}\right)$$`
- Activation Function: logistic `$$\sigma(v)=\frac{1}{1+e^{-v}}$$`
- Feature visualization optimization:

`$$\arg\underset{PC}{\max} \ \sigma\left(\hat{\beta}_{0,i} + \hat{\beta}_{1,i}PC_1 +\cdots+\hat{\beta}_{5,i}PC_5\right)$$`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
